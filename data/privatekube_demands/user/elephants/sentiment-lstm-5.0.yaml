accuracy: 0.760106283507935
adaptive_batch_size: 1
alphas:
- 1.5
- 1.75
- 2
- 2.5
- 3
- 4
- 5
- 6
- 8
- 16
- 32
- 64
batch_size: 179
best_alpha: 8.0
delta: 1.0e-09
device: cuda
dp: 1
dp_eval: 0
dropout: 0.25
dynamic_clipping: 0
embedding_dim: 100
epsilon: 4.965206439174252
hidden_dim: 40
hidden_dim_1: 240
hidden_dim_2: 195
learning_rate: 0.01
learning_rate_scheduler: 1
loss: 0.7035571006062913
max_grad_norm: 1.0
max_text_len: 50
model: lstm
n_blocks: 1000
n_blocks_test: 200
n_epochs: 60
n_train_users: 32104
n_trainable_parameters: 22761
n_workers: 6
noise: 1.0704752349853517
non_dp_batch_size: 256
per_layer_clipping: 0
rdp_epsilons:
- 0.3469687183333038
- 0.4059084652667478
- 0.4651788701436762
- 0.5847329002439696
- 0.7056751952051806
- 0.9519194562284551
- 1.2043442893095775
- 1.4634710841394463
- 2.0047398910390504
- 15534.279301260629
- 92426.99606719706
- 243298.66905814415
target_epsilon: 5.0
task: sentiment
test_size: 39077
timeframe_days: 0
total_time: 732.7283742427826
train_size: 297910
training_accuracy_epochs:
- 0.6582815553888929
- 0.6626509591187845
- 0.7179551011357228
- 0.7258200209234014
- 0.7443899782010297
- 0.762679045426779
- 0.7663306171001669
- 0.7680471679351849
- 0.7704815478298251
- 0.7749757906578106
- 0.7764738704905164
- 0.7767859697341919
- 0.7772229100738823
- 0.7765987109205577
- 0.777098071974749
- 0.7770044410694911
- 0.7770980709757884
- 0.7772541195986657
- 0.7771604920232762
- 0.7758496686732969
- 0.7775974310310193
- 0.7774725915999386
- 0.7775350089845711
- 0.7775662208402623
- 0.7770356505942744
- 0.7780343707047361
- 0.7762554001541777
- 0.7750694215630686
- 0.7765050796823129
- 0.7767859704001656
- 0.7760369288188785
- 0.7773477511698973
- 0.7774413797442473
- 0.7778471102261676
- 0.7761929794396768
- 0.7760369291518654
- 0.7768795999734761
- 0.7763178215346522
- 0.776224189297447
- 0.7771604916902893
- 0.7773789616936412
- 0.7771916992171517
- 0.7768795989745156
- 0.7787522014292925
- 0.7765362918709909
- 0.7760993505323399
- 0.7771917018810464
- 0.7767235543474805
- 0.7761305607230969
- 0.7774725892690307
- 0.7774101692205034
- 0.776848392113627
- 0.7769732298797736
- 0.7764114501090024
- 0.7781280012770072
- 0.7763178212016655
- 0.7771604910243157
- 0.7765050806812734
- 0.777285331454357
- 0.7785337320919143
training_loss_epochs:
- 0.6852029628593829
- 0.7170841324262779
- 0.7217815198711843
- 0.7136170457861277
- 0.6967900287172648
- 0.6482791682528384
- 0.6555580350273814
- 0.6593612054539792
- 0.6608853450034584
- 0.6551585458843402
- 0.6534104673556109
- 0.6571099891342931
- 0.6549977666838875
- 0.6549764413740382
- 0.6599031352131061
- 0.6581522597613947
- 0.6559263908996262
- 0.6554591384346925
- 0.6560857222732885
- 0.6592136025428772
- 0.6525417239972333
- 0.6578551424282223
- 0.6556435673596472
- 0.6570747254947045
- 0.6581162841959373
- 0.6562802490242367
- 0.6579497248433822
- 0.6595762988042565
- 0.6582464490855873
- 0.6582026974448945
- 0.6611194444102282
- 0.6540133267141587
- 0.6538382310774074
- 0.65403121826369
- 0.6581271405659574
- 0.6577556616101186
- 0.6591458598661689
- 0.6571063271114946
- 0.6587748657391724
- 0.6533805914431311
- 0.6546516796397097
- 0.6572373547700531
- 0.657078119962575
- 0.6520292689014413
- 0.6567787449786117
- 0.6573574776090058
- 0.6560186562258438
- 0.6568907590884736
- 0.6579388469957107
- 0.6562154952374251
- 0.6551173800529715
- 0.6563840661634947
- 0.6556125714459233
- 0.658363697582117
- 0.6546890797228787
- 0.6567076862524341
- 0.6568767198637211
- 0.6586411513762767
- 0.6557652181087259
- 0.6529503777706424
training_time: 703.3035178184509
user_level: 1
validation_accuracy_epochs:
- 0.6532225608825684
- 0.6812525661024329
- 0.7089847791684817
- 0.7218726473311855
- 0.7421610600327793
- 0.7507527846179597
- 0.7517188240403998
- 0.7534251898935397
- 0.7553073527061775
- 0.756860662813056
- 0.7580851202141748
- 0.7585071357962203
- 0.758773877196116
- 0.7586125070101595
- 0.7580995412722026
- 0.7578577554389222
- 0.7580768008754678
- 0.7588110353848706
- 0.7588875628497502
- 0.7587428207266821
- 0.7586601948084897
- 0.7589247226715088
- 0.7588648224530155
- 0.7586829319392165
- 0.7586291383390558
- 0.7587882966211398
- 0.758901983091276
- 0.7588792418780392
- 0.7588337692495895
- 0.7588110370178746
- 0.7586746109675054
- 0.758742824809192
- 0.7586291407885617
- 0.7588337749651034
- 0.7586064061073408
- 0.7588110337518665
- 0.758901983091276
- 0.7588337692495895
- 0.7586518754697826
- 0.7586746158665174
- 0.7585381947151603
- 0.7585836689766139
- 0.7586973480982323
- 0.7587655594904129
- 0.7586291416050637
- 0.7589247194055009
- 0.7587200868619631
- 0.7587428207266821
- 0.7586291399720597
- 0.75874282399269
- 0.7587655603069149
- 0.7587882933551318
- 0.7585836624445981
- 0.7588110345683686
- 0.7587200844124572
- 0.7587200860454612
- 0.7585381971646662
- 0.7587428199101801
- 0.7588565039308104
- 0.7588565055638143
validation_loss_epochs:
- 0.6929056848565193
- 0.7923043190616451
- 0.7447318855213793
- 0.7720572164613907
- 0.6856877942607827
- 0.6929855795755778
- 0.6890406665736681
- 0.713806492825077
- 0.7026026893968451
- 0.7161292543966477
- 0.7084888709734564
- 0.7115726021871175
- 0.7098556299732156
- 0.7125356360657574
- 0.7118762568251727
- 0.711848988924941
- 0.7108617890371035
- 0.710024261311309
- 0.7099998189161901
- 0.7102646407199232
- 0.7107842103259204
- 0.7098270262757392
- 0.7101068345651235
- 0.7106052359489545
- 0.7106437723930568
- 0.7103521725902818
- 0.7099103878622186
- 0.7098729145036985
- 0.7101431815591577
- 0.7100964984665178
- 0.7103358303030877
- 0.7103438585588376
- 0.7106156626792803
- 0.7099856648543109
- 0.7107280860208485
- 0.7101360314512906
- 0.7097572667957985
- 0.710059015309974
- 0.7106791865335752
- 0.7103434899081923
- 0.7108715511348149
- 0.7105052046579857
- 0.710492341485742
- 0.7100950136576614
- 0.7106758911315709
- 0.7097540399799608
- 0.7102761550308907
- 0.7101376301621738
- 0.7104061858294761
- 0.7104464973488899
- 0.7100916370137097
- 0.7102083098398496
- 0.7108134390556649
- 0.7102146805965737
- 0.7103559227838908
- 0.7101354999084996
- 0.7109587576291333
- 0.7102006224736775
- 0.710017495367625
- 0.7100235746331411
virtual_batch_multiplier: 0
vocab_size: 10000
