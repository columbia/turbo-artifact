accuracy: 0.6162156579287156
adaptive_batch_size: 1
alphas:
- 1.5
- 1.75
- 2
- 2.5
- 3
- 4
- 5
- 6
- 8
- 16
- 32
- 64
batch_size: 428
best_alpha: 64.0
delta: 1.0e-09
device: cuda
dp: 1
dp_eval: 0
dropout: 0.25
dynamic_clipping: 0
embedding_dim: 100
epsilon: 0.4982638182080895
hidden_dim: 40
hidden_dim_1: 240
hidden_dim_2: 195
learning_rate: 0.01
learning_rate_scheduler: 1
loss: 1.3437641275965648
max_grad_norm: 1.0
max_text_len: 30
model: lstm
n_blocks: 10000
n_blocks_test: 200
n_epochs: 60
n_train_users: 183808
n_trainable_parameters: 23171
n_workers: 6
noise: 5.197936401367188
non_dp_batch_size: 256
per_layer_clipping: 0
rdp_epsilons:
- 0.003946472215261565
- 0.004604319639156529
- 0.0052621963978546635
- 0.006578037204018518
- 0.007893994701642227
- 0.0105262599629736
- 0.01315899234976433
- 0.015792192029353693
- 0.021059993937148247
- 0.04214992683334204
- 0.08441995423318889
- 0.16932309063751158
target_epsilon: 0.5
task: product
test_size: 39077
timeframe_days: 0
total_time: 1085.6233472824097
train_size: 2545758
training_accuracy_epochs:
- 0.4989760859515561
- 0.5572239107741065
- 0.5665642571893883
- 0.5708068995486885
- 0.5744940227978713
- 0.5812310548651191
- 0.5813781029972441
- 0.5860074304200552
- 0.5899450824810908
- 0.5912576333070413
- 0.5920037725866536
- 0.5923087617734095
- 0.5884528047554977
- 0.5922270689533029
- 0.5938609484192374
- 0.5921126977824942
- 0.5978857398311019
- 0.6027274697254865
- 0.6033537928318922
- 0.6072206423554943
- 0.604410369357307
- 0.6107716082415103
- 0.6121604061904765
- 0.6165827754494193
- 0.6197742894812898
- 0.6201119568798091
- 0.6183800443069084
- 0.6191370755364567
- 0.616713485795579
- 0.6182438871521494
- 0.6211576403437794
- 0.620901665626428
- 0.6221216282088717
- 0.6238099708979502
- 0.6245343255829978
- 0.6263588261215282
- 0.6269851438093297
- 0.6291255278465075
- 0.6279709204371436
- 0.6293923963199962
- 0.630029609709075
- 0.6312332347834305
- 0.6304544174309933
- 0.631146093070646
- 0.6323007024251498
- 0.6326002464705692
- 0.6331448752285439
- 0.6339618138500027
- 0.6348168765192543
- 0.6350456208060116
- 0.6361403213792192
- 0.6356719407168302
- 0.6369844922374734
- 0.6378885728495938
- 0.6369245823446688
- 0.6377360777699308
- 0.6384331975783502
- 0.6390704123568146
- 0.6393046022017241
- 0.6392011224250971
training_loss_epochs:
- 1.7584520989642554
- 1.6266024963005439
- 1.6013072367870447
- 1.582696432833905
- 1.564915315810339
- 1.5357412850106513
- 1.519894206440532
- 1.49546469230474
- 1.5021822705413357
- 1.4825782625825255
- 1.4647365091563938
- 1.4831100207664472
- 1.4765942027240921
- 1.4495025917644546
- 1.450805103584325
- 1.4506474958433138
- 1.4360008587092508
- 1.4254042299199494
- 1.416923384566407
- 1.4135822360610073
- 1.423567622135847
- 1.3913928023029318
- 1.4047331154207527
- 1.4033234247238764
- 1.390610132461939
- 1.3970652863140152
- 1.4018504780767125
- 1.3937310491686379
- 1.3940660600617771
- 1.410558286524597
- 1.389004079611985
- 1.3813978617007916
- 1.3695672289196983
- 1.3659124374389648
- 1.352142121408369
- 1.344984637273775
- 1.3428104318383134
- 1.3412567332630112
- 1.3363129036687749
- 1.3364905846980346
- 1.3370151441969793
- 1.3281971849761642
- 1.329721212387085
- 1.3294907494302675
- 1.3267057531085604
- 1.321975080561249
- 1.3245093641859111
- 1.3265529940456222
- 1.3241365795091038
- 1.314263278232032
- 1.3129194266312605
- 1.3107540807523927
- 1.3081853509227157
- 1.3076518796262764
- 1.2999915709862342
- 1.3040773726843453
- 1.302043379325689
- 1.2952569750916985
- 1.300248822687945
- 1.2945217388771075
training_time: 1028.4540927410126
user_level: 1
validation_accuracy_epochs:
- 0.5084648507256662
- 0.5271418517635714
- 0.5472938178047058
- 0.5464355820609678
- 0.5475361366425792
- 0.5570472575003101
- 0.5539644052905421
- 0.5619608423402233
- 0.5570780519516237
- 0.5598059354289886
- 0.5644115690262087
- 0.5644196694897067
- 0.5658135942874416
- 0.567841274123038
- 0.5653767778027442
- 0.5686395418259406
- 0.5709849038431721
- 0.5755297580072957
- 0.5795907820424726
- 0.5857945661390981
- 0.5838122752404982
- 0.5852661728858948
- 0.586841631320215
- 0.5945074250621181
- 0.5949888075551679
- 0.5927885117069367
- 0.5951857451469668
- 0.5989987830961904
- 0.5968738563599125
- 0.6008295339922751
- 0.6036117115328389
- 0.6041247094831159
- 0.6052163474021419
- 0.6057674384886219
- 0.6038378150232376
- 0.6071313792659391
- 0.6063023151889924
- 0.6051272019263236
- 0.6099054524975438
- 0.6079539522047965
- 0.6096712446981861
- 0.6092563060022169
- 0.6090990862538738
- 0.6096939329178103
- 0.60974742327967
- 0.6106210600945258
- 0.6103268707952192
- 0.6133489358809686
- 0.6121738264637608
- 0.6117443057798571
- 0.6106810242899002
- 0.6099581314671424
- 0.612822167335018
- 0.6113212627749289
- 0.614661826241401
- 0.6109225346196082
- 0.6111040711402893
- 0.6127249156275103
- 0.6121438414819779
- 0.6141188375411495
validation_loss_epochs:
- 1.7179822575661443
- 1.704251762359373
- 1.6525510780272945
- 1.6176205142851798
- 1.6025437654987458
- 1.5972600560034476
- 1.523894367679473
- 1.5797001315701393
- 1.5794254233760219
- 1.5275586458944506
- 1.5079812349811677
- 1.516364655187053
- 1.5089406928708475
- 1.5181431347324001
- 1.5180275209488407
- 1.4756019000084168
- 1.4796657946801954
- 1.485909850366654
- 1.4774374884943808
- 1.4429873958710702
- 1.493493983822484
- 1.44440625175353
- 1.492102034630314
- 1.4615452635672785
- 1.4421327652469758
- 1.4516458049897225
- 1.4794711451376639
- 1.4340885800700034
- 1.4867855771895377
- 1.4460538625717163
- 1.4364189063349078
- 1.4196782919668383
- 1.4188785822160783
- 1.408816599076794
- 1.4080873304797756
- 1.3991139473453644
- 1.40032063376519
- 1.394069460130507
- 1.394251658070472
- 1.4027394748503161
- 1.3990039133256482
- 1.39637759424025
- 1.3884381286559566
- 1.397401144427638
- 1.3790338000943583
- 1.3944580324234501
- 1.384793835301553
- 1.390898908338239
- 1.3784113468662385
- 1.3745044239105717
- 1.3768844912129063
- 1.3769609735858055
- 1.3774808722157632
- 1.3714734085144535
- 1.3626718521118164
- 1.3711252020251365
- 1.3699325092377201
- 1.3710273542711813
- 1.3658834195906115
- 1.3656832248933855
virtual_batch_multiplier: 0
vocab_size: 10000
