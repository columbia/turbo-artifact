accuracy: 0.7816219614899677
adaptive_batch_size: 1
alphas:
- 1.5
- 1.75
- 2
- 2.5
- 3
- 4
- 5
- 6
- 8
- 16
- 32
- 64
batch_size: 428
best_alpha: 64.0
delta: 1.0e-09
device: cuda
dp: 1
dp_eval: 0
dropout: 0.25
dynamic_clipping: 0
embedding_dim: 100
epsilon: 0.9947741547318798
hidden_dim: 40
hidden_dim_1: 240
hidden_dim_2: 195
learning_rate: 0.01
learning_rate_scheduler: 1
loss: 0.6418095762315004
max_grad_norm: 1.0
max_text_len: 50
model: lstm
n_blocks: 10000
n_blocks_test: 200
n_epochs: 60
n_train_users: 183808
n_trainable_parameters: 22761
n_workers: 6
noise: 2.7107786560058598
non_dp_batch_size: 256
per_layer_clipping: 0
rdp_epsilons:
- 0.015256286133933372
- 0.01780057977669003
- 0.02034532508181092
- 0.025436171304554327
- 0.03052882623359153
- 0.04071956736920023
- 0.05091755920515333
- 0.061122812483012955
- 0.08155514646019921
- 0.1635771062970585
- 0.3290439771011497
- 0.6658334271613018
target_epsilon: 1.0
task: sentiment
test_size: 39077
timeframe_days: 0
total_time: 1637.1326370239258
train_size: 2545758
training_accuracy_epochs:
- 0.6505511422535201
- 0.7009999114316661
- 0.7500489926560497
- 0.7567751322870766
- 0.770189288890723
- 0.7745898717488998
- 0.770461602922364
- 0.7693505624791125
- 0.7669542045582146
- 0.7630165534697491
- 0.769546627998352
- 0.7710661374049864
- 0.7730540234447915
- 0.7742413110110588
- 0.7744591618195559
- 0.7759187609165699
- 0.7764252638205503
- 0.7782715489536455
- 0.7783695820606116
- 0.7796113318774528
- 0.7809293256217228
- 0.7822800018848517
- 0.782378035269695
- 0.7838485268723993
- 0.7847035902363437
- 0.7855913318120517
- 0.7855368683110306
- 0.7850521516688775
- 0.7849704573204467
- 0.7865335361940877
- 0.787099948018303
- 0.7870236999227173
- 0.7878188547832427
- 0.7876445746921993
- 0.78760100355793
- 0.7876772527650242
- 0.7876010036968685
- 0.7882763403001087
- 0.7881347381429517
- 0.7881674173272851
- 0.7879168878902089
- 0.7874811865510918
- 0.787797069216108
- 0.7879604587466011
- 0.7876173423164652
- 0.7874539551479277
- 0.7882600004300648
- 0.7881401848126125
- 0.7878025168583388
- 0.787138071371403
- 0.7882545552887283
- 0.7879876891771952
- 0.7881728620518059
- 0.7885486554432583
- 0.7882763416894943
- 0.7873177988268001
- 0.7882545575117453
- 0.7883416956121271
- 0.7879767972272593
- 0.7875955571661463
training_loss_epochs:
- 0.7009917005792364
- 0.7471664932780054
- 0.7035788179555417
- 0.6816201631005827
- 0.6642972367765743
- 0.648437575860457
- 0.6656842654123729
- 0.6666507965479141
- 0.657696346997659
- 0.6668527558411196
- 0.659983509626144
- 0.655932326833685
- 0.6497642614624717
- 0.6459645400375197
- 0.6447638935281402
- 0.6428192266217478
- 0.6419236950385265
- 0.6388385049946659
- 0.6379606134685881
- 0.6323074631062977
- 0.6311451642663328
- 0.6343925635575692
- 0.6297795843411159
- 0.6288622334981576
- 0.6268458207309385
- 0.6264563284831725
- 0.626803195768303
- 0.6299409118843523
- 0.6327981229150768
- 0.628478974451274
- 0.6293699181441105
- 0.6302056084007094
- 0.6266619333159396
- 0.6267260608973203
- 0.6299857547233155
- 0.6280433602961071
- 0.6294972346379206
- 0.6272675474901577
- 0.6279795810337111
- 0.6277102022182136
- 0.6274503503507112
- 0.6293689539104631
- 0.6277747525201811
- 0.6278430837573428
- 0.6290805603796508
- 0.6285917993053134
- 0.6268181374995581
- 0.6278046981576995
- 0.628989105180149
- 0.6301874940767711
- 0.6271828376190923
- 0.6288852130338585
- 0.6282245456338762
- 0.6253077262765044
- 0.6270796482935375
- 0.6312399053629184
- 0.6269316967133876
- 0.6264125417162488
- 0.6276422246491714
- 0.6298067664905583
training_time: 1577.0633027553558
user_level: 1
validation_accuracy_epochs:
- 0.6528432638414444
- 0.746296344264861
- 0.7498532898964421
- 0.7531006528485206
- 0.7659750311605392
- 0.7677158155748921
- 0.7671428457383187
- 0.7715296610709159
- 0.7572621780057107
- 0.7637212449504484
- 0.7663899660110474
- 0.7685456891213694
- 0.76808536629523
- 0.7679338205245233
- 0.7676858325158397
- 0.7669856202217841
- 0.7703083580540072
- 0.7702621606088453
- 0.7739033987445216
- 0.7747859454924061
- 0.7748232233908868
- 0.7742956338390228
- 0.7801898429470677
- 0.7790819925646628
- 0.7787424306715688
- 0.7830919604147634
- 0.7803097847969301
- 0.776752031618549
- 0.7815618861106134
- 0.7808000945275829
- 0.779541507844002
- 0.7801379753697303
- 0.7815464888849566
- 0.7811015709753959
- 0.7819987093248675
- 0.7820287000748419
- 0.7822928909332522
- 0.7820286943066505
- 0.7817572028406204
- 0.7830319866057365
- 0.7811915259207448
- 0.7812668969554286
- 0.7821494437033131
- 0.7824436233889672
- 0.7830319904511974
- 0.7813649562097364
- 0.7813649562097364
- 0.7811688319329293
- 0.78175720091789
- 0.7826397515112354
- 0.7822475010348905
- 0.780874652247275
- 0.781364958132467
- 0.7828358738653122
- 0.7820513806035442
- 0.7828358815562341
- 0.7812668950326981
- 0.7815610804865437
- 0.7810707746013519
- 0.7819533232719668
validation_loss_epochs:
- 0.7397532366937206
- 0.722109200492982
- 0.688980475548775
- 0.7046230416144094
- 0.6481456679682578
- 0.6743281483650208
- 0.6909099663457563
- 0.6605579756921337
- 0.6712314371139773
- 0.6648944193317045
- 0.6623460765807859
- 0.6541029003358656
- 0.6497125587155742
- 0.6576412954638081
- 0.6566557557352127
- 0.6588950618620841
- 0.6527866028970287
- 0.6630870219199888
- 0.6446828303798553
- 0.6418383217627003
- 0.6490525930158554
- 0.6512295401865437
- 0.6356643803658024
- 0.6434184793503054
- 0.6432002513639389
- 0.6300924401129445
- 0.6401365764679448
- 0.658038156647836
- 0.6394164350724989
- 0.6406686479045499
- 0.645894133275555
- 0.6391590006889836
- 0.6378024239693919
- 0.6394801639741466
- 0.6373416569925123
- 0.6359564238978971
- 0.6357844548840677
- 0.6369374375189504
- 0.6366547903706951
- 0.6349204048033683
- 0.6388586605748823
- 0.6388040473384242
- 0.635813424664159
- 0.6353847490202996
- 0.6344578102711709
- 0.6381660988253932
- 0.6398771328310813
- 0.6391686547187067
- 0.6376177226343462
- 0.6349507454902895
- 0.6361918853175256
- 0.6402394867712452
- 0.6383449954371299
- 0.6361712755695466
- 0.6368794518132364
- 0.634842368864244
- 0.638592133598943
- 0.6381987544798082
- 0.6397005665686822
- 0.6370898658229459
virtual_batch_multiplier: 0
vocab_size: 10000
