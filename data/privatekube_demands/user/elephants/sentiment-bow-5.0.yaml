accuracy: 0.7080218756900114
adaptive_batch_size: 1
alphas:
- 1.5
- 1.75
- 2
- 2.5
- 3
- 4
- 5
- 6
- 8
- 16
- 32
- 64
batch_size: 96
best_alpha: 8.0
delta: 1.0e-09
device: cuda
dp: 1
dp_eval: 0
dropout: 0.25
dynamic_clipping: 0
embedding_dim: 100
epsilon: 4.976605521613281
hidden_dim: 100
hidden_dim_1: 240
hidden_dim_2: 195
learning_rate: 0.01
learning_rate_scheduler: 1
loss: 0.5755132177442897
max_grad_norm: 1.0
max_text_len: 50
model: bow
n_blocks: 100
n_blocks_test: 200
n_epochs: 60
n_train_users: 9304
n_trainable_parameters: 101
n_workers: 6
noise: 1.3222427368164062
non_dp_batch_size: 256
per_layer_clipping: 0
rdp_epsilons:
- 0.3532184091699749
- 0.4130878432007284
- 0.47325184405240706
- 0.5944776994634209
- 0.7169250901842574
- 0.965607891794834
- 1.2195626197889202
- 1.4790797043965296
- 2.0161389734780792
- 10.03561858248085
- 25518.061121794406
- 78663.00569836024
target_epsilon: 5.0
task: sentiment
test_size: 39077
timeframe_days: 0
total_time: 65.56359767913818
train_size: 29727
training_accuracy_epochs:
- 0.6282552272702256
- 0.6362847449878851
- 0.632704014579455
- 0.6330295310666164
- 0.6346571370959282
- 0.6402995015184084
- 0.6457248460501432
- 0.6529948102931181
- 0.6625434222320715
- 0.6718750186264515
- 0.6665581787625948
- 0.6750217216710249
- 0.6773003693670034
- 0.6797960251569748
- 0.6890191199878851
- 0.6870659918834766
- 0.6875000186264515
- 0.69194880562524
- 0.6920573146392902
- 0.6987847437461218
- 0.6986762378364801
- 0.7008463690678278
- 0.7000868252168099
- 0.7010633864750465
- 0.7043185954292616
- 0.7043185979127884
- 0.7031250217308601
- 0.7006293628364801
- 0.7004123466710249
- 0.705078144868215
- 0.7006293603529533
- 0.7016059228529533
- 0.7034505400806665
- 0.6993272776405016
- 0.6998698109139999
- 0.703125019868215
- 0.7017144293834766
- 0.7043185960501432
- 0.7033420347919067
- 0.7025824878364801
- 0.7031250180055698
- 0.7041015817473332
- 0.7018229346722364
- 0.6984592216710249
- 0.7030165127168099
- 0.7032335257778565
- 0.7016059247155985
- 0.7011718942473332
- 0.7041015817473332
- 0.7056206781417131
- 0.702039947733283
- 0.7026909900208315
- 0.705512173473835
- 0.70366755562524
- 0.7055121747155985
- 0.7065972406417131
- 0.7019314436862866
- 0.7027994984139999
- 0.7050781461099783
- 0.7009548805654049
training_loss_epochs:
- 0.693229716271162
- 0.6672880550225576
- 0.6585026824225982
- 0.6464584382871786
- 0.6373430198679367
- 0.6316697088380655
- 0.6233133201797804
- 0.6182498000562191
- 0.6083713589857022
- 0.60228649713099
- 0.6074396443242828
- 0.5979054557780424
- 0.5948361922055483
- 0.5931536325563987
- 0.5896203670029839
- 0.5874569198737541
- 0.5869734150667986
- 0.5862992865343889
- 0.5837623787422975
- 0.5774279370283087
- 0.5813700367386142
- 0.5787968263030052
- 0.5783016461258134
- 0.5815656054764986
- 0.5745895399401585
- 0.5737694228688875
- 0.5771009183178345
- 0.5762033925081292
- 0.5783369550481439
- 0.5746844050784906
- 0.5798303410410881
- 0.5775272337098917
- 0.5758793897305926
- 0.5799278806274136
- 0.5766725161423286
- 0.5755188347150882
- 0.5777313867583871
- 0.5743866451084614
- 0.5735351198042432
- 0.5754524559403459
- 0.5794566168139378
- 0.5753782090420524
- 0.5757730991269151
- 0.5782899276042978
- 0.5784329740951458
- 0.5752560012042522
- 0.5775448869292935
- 0.577320825929443
- 0.5751169417053461
- 0.5744111950819691
- 0.5771834319457412
- 0.5779854916036129
- 0.5745564972360929
- 0.5770404543727636
- 0.5754487765952945
- 0.5736427661031485
- 0.5773516794045767
- 0.5767290132741133
- 0.5755700847754875
- 0.576072990273436
training_time: 42.97150230407715
user_level: 1
validation_accuracy_epochs:
- 0.6536945946076337
- 0.6534160752506817
- 0.6540148950674954
- 0.6596967100220568
- 0.6563962439403814
- 0.6711508644854322
- 0.6654969011159504
- 0.6834545100436491
- 0.6788798106067321
- 0.6683308464639327
- 0.6833709526587936
- 0.6835589566651512
- 0.6924576842609573
- 0.6953751856789869
- 0.6938085087081965
- 0.6876392807154095
- 0.6992883989039589
- 0.6970463075182017
- 0.7070939361172563
- 0.6970532720579821
- 0.7047613276278272
- 0.7052069626310292
- 0.7035010229138767
- 0.70565955559997
- 0.7057779269183383
- 0.7056874067467802
- 0.7060425228932324
- 0.7059032618999481
- 0.7060146713081528
- 0.7057640013449332
- 0.7064324503435808
- 0.7060216354096637
- 0.7063280054751564
- 0.7059659291716183
- 0.7059450416880495
- 0.7057361475685063
- 0.7058405924369308
- 0.7059798556215623
- 0.7058057806947652
- 0.7060494826120489
- 0.7060494848033961
- 0.7058405941900086
- 0.7060146699933445
- 0.705840594628278
- 0.7059450373053551
- 0.7059450394967023
- 0.7060843004899866
- 0.705840593751739
- 0.7055968896431082
- 0.7059798551832929
- 0.7058754120679462
- 0.7060842974221005
- 0.7059450399349717
- 0.7059102273162674
- 0.7059798556215623
- 0.7058405933134696
- 0.7059450390584329
- 0.7059450412497801
- 0.7058057815713041
- 0.7059798547450233
validation_loss_epochs:
- 0.6581919969004744
- 0.6669781133532524
- 0.6315190231098848
- 0.6178809150176889
- 0.6275206234525231
- 0.6067888894501854
- 0.6143314251128364
- 0.597177663270165
- 0.5973566268735072
- 0.619906783761347
- 0.5958713560419924
- 0.5955819163252326
- 0.5925958980532253
- 0.5896397923283717
- 0.5905871400061775
- 0.5984444876804071
- 0.588545302476953
- 0.5902397314853528
- 0.579493831624003
- 0.5881025405491099
- 0.5840203444309094
- 0.5813820447553607
- 0.5852414803469882
- 0.5817736431079752
- 0.5813010481788832
- 0.5814625237356214
- 0.5809503681957722
- 0.5805952312315211
- 0.5802870754371671
- 0.5818592423901838
- 0.5813681084443542
- 0.5815674320739859
- 0.5812218695440713
- 0.5808695550350582
- 0.5809825573335675
- 0.5811632510055514
- 0.580987184363253
- 0.5809391715070781
- 0.5812358051976737
- 0.5808289221980992
- 0.5809147651581203
- 0.5810592737706268
- 0.5809366400627529
- 0.5811163509155021
- 0.5810191030887997
- 0.5810688940041205
- 0.580753860228202
- 0.581009184174678
- 0.5812625600134625
- 0.5808639589916257
- 0.5810602129820515
- 0.5809350517742774
- 0.5811012531904614
- 0.5810464517596889
- 0.5808959031368003
- 0.5811519923017305
- 0.5810459519133848
- 0.5810790912193411
- 0.5810973385677618
- 0.5810453352682731
virtual_batch_multiplier: 0
vocab_size: 10000
