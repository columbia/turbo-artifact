accuracy: 0.609061616018784
adaptive_batch_size: 1
alphas:
- 1.5
- 1.75
- 2
- 2.5
- 3
- 4
- 5
- 6
- 8
- 16
- 32
- 64
batch_size: 323
best_alpha: 64.0
delta: 1.0e-09
device: cuda
dp: 1
dp_eval: 0
dropout: 0.25
dynamic_clipping: 0
embedding_dim: 100
epsilon: 0.9941723967623615
hidden_dim: 100
hidden_dim_1: 185
hidden_dim_2: 150
learning_rate: 0.01
learning_rate_scheduler: 1
loss: 1.6511118412017822
max_grad_norm: 1.0
max_text_len: 60
model: feedforward
n_blocks: 5000
n_blocks_test: 200
n_epochs: 60
n_train_users: 104866
n_trainable_parameters: 48246
n_workers: 6
noise: 3.0922445678710937
non_dp_batch_size: 256
per_layer_clipping: 0
rdp_epsilons:
- 0.015246720709001238
- 0.017789400585570547
- 0.0203325266596955
- 0.025420117700272048
- 0.030509495073951044
- 0.0406936139769089
- 0.05088489372168824
- 0.06108334468557398
- 0.0815018019011586
- 0.16346457177153687
- 0.32879474723512436
- 0.6652316691917836
target_epsilon: 1.0
task: product
test_size: 39077
timeframe_days: 0
total_time: 163.16613483428955
train_size: 1332804
training_accuracy_epochs:
- 0.5241753486111577
- 0.6152199501240695
- 0.6263903059341289
- 0.6331651405787763
- 0.6312349335646924
- 0.6302029415045256
- 0.6334135834081673
- 0.63628978162636
- 0.6395290891329447
- 0.6416121826127723
- 0.6434181685800906
- 0.6440488306092628
- 0.6446986047574031
- 0.6453961540519455
- 0.6456828183597989
- 0.644689048697919
- 0.6451381551630703
- 0.6447846013822673
- 0.6444979385461336
- 0.6445170488254524
- 0.6451477123263442
- 0.6443163829821127
- 0.64441193971369
- 0.6451763779292872
- 0.6450043793445752
- 0.6441921645108565
- 0.645176378481182
- 0.6444692718394008
- 0.6446030472899661
- 0.6443832741107469
- 0.6447272694405214
- 0.6445934915984118
- 0.6449757122699126
- 0.6455681502451132
- 0.6446317152844535
- 0.6435519455023754
- 0.6449183806960965
- 0.64513815626686
- 0.6453770423009072
- 0.6453770417490123
- 0.6447750480822575
- 0.6445934941739212
- 0.6449566025424887
- 0.6442303843336341
- 0.6448228247003791
- 0.6449757115340528
- 0.645510819223192
- 0.6449852675935368
- 0.6446126066608193
- 0.6439532771890546
- 0.6455872621801164
- 0.6444692725752607
- 0.6448801582978095
- 0.6451668211339433
- 0.6449661567623233
- 0.6436570564537872
- 0.6446603818072213
- 0.6439723863645836
- 0.6447654907350187
- 0.6448323787362488
training_loss_epochs:
- 1.61477850984644
- 1.408871752244455
- 1.443804270323412
- 1.5058210536285683
- 1.6015898831832556
- 1.689744355501952
- 1.6486514184945895
- 1.607498253201261
- 1.575585506580494
- 1.550458988657704
- 1.5342512583291088
- 1.5220483949890844
- 1.520538059649644
- 1.5156640770994587
- 1.5171328845583363
- 1.5198514969260604
- 1.5139055678873887
- 1.517060695974915
- 1.5156515111893782
- 1.5158537026540733
- 1.5172961604448012
- 1.5144501081955286
- 1.5176518805968908
- 1.5092636192286457
- 1.5149311940243215
- 1.516020859465187
- 1.5146081690435056
- 1.5145282204504367
- 1.5137784205101155
- 1.5160869981771634
- 1.5114839095392345
- 1.5148129293948045
- 1.516234866262954
- 1.5081831812858582
- 1.5126204630474986
- 1.5204178407604312
- 1.5190864359890972
- 1.5120481591165802
- 1.510377104267662
- 1.5113489977371546
- 1.5159807907946317
- 1.514368024505215
- 1.5150410606537337
- 1.515233529570662
- 1.5125642632260734
- 1.5127401999485346
- 1.5133127640058965
- 1.5154236908312197
- 1.5160392949610582
- 1.5177950851711226
- 1.514040916790197
- 1.5148900239555925
- 1.5166002339051095
- 1.5141447223263023
- 1.5168711690255154
- 1.5193301019845185
- 1.5166682593616438
- 1.5193872911694608
- 1.5125001381944727
- 1.5150437229945335
training_time: 120.37472987174988
user_level: 1
validation_accuracy_epochs:
- 0.5706353754532046
- 0.5926235012891816
- 0.5969027266269777
- 0.5961654200786497
- 0.6005878332184582
- 0.5985027159132609
- 0.5982327272252339
- 0.6012788356804266
- 0.6050900322634999
- 0.6085564421444405
- 0.6068018733001337
- 0.605417722608985
- 0.6088007921125831
- 0.6096420985896412
- 0.6074779120887198
- 0.6080891257379113
- 0.6083982950303612
- 0.609171227711003
- 0.6084773729487163
- 0.6080891271916832
- 0.6080136168293837
- 0.6084773744024882
- 0.6083227861218337
- 0.6077044460831619
- 0.6089411232529617
- 0.6086319612293709
- 0.6092502954529553
- 0.607859030002501
- 0.610641557995866
- 0.6090957115336162
- 0.6078590329100446
- 0.6086319554142836
- 0.607859030002501
- 0.608941129068049
- 0.6075498636175947
- 0.6090957071723008
- 0.6080136182831555
- 0.6083227861218337
- 0.6098686369453988
- 0.6084773729487163
- 0.6087865393336226
- 0.60863195250674
- 0.6087865393336226
- 0.6083227846680618
- 0.6087865407873945
- 0.6095594632916335
- 0.6069315235789229
- 0.6098686354916271
- 0.6090957071723008
- 0.608941129068049
- 0.6084773729487163
- 0.6081682036562663
- 0.6089411276142772
- 0.6067769323907247
- 0.6077044475369338
- 0.6080136153756118
- 0.6095594661991771
- 0.6083227861218337
- 0.6080136153756118
- 0.6086319583218273
validation_loss_epochs:
- 1.5103572287210605
- 1.4563028812408447
- 1.5668850468426216
- 1.6261454588029443
- 1.769931813565696
- 1.8890785705752489
- 1.7992022793467453
- 1.710764236566497
- 1.6791863703146213
- 1.672802919294776
- 1.6417100080629674
- 1.6468042280615829
- 1.6307098836433598
- 1.6276601145907146
- 1.6383615499589501
- 1.6349658791611834
- 1.6383605904695464
- 1.6329274061249524
- 1.6339062336014538
- 1.638000604582996
- 1.638690215785329
- 1.6368853929566174
- 1.635115879337962
- 1.637809273673267
- 1.633906486557751
- 1.6321315532777367
- 1.6306783891305692
- 1.6413800338419473
- 1.6288726649633267
- 1.6289684685265147
- 1.635907315626377
- 1.6353063292619658
- 1.63562150408582
- 1.6334095146597885
- 1.6365967349308292
- 1.6280359727580374
- 1.6367279203926646
- 1.6379183443581187
- 1.6287466665593588
- 1.6373412609100342
- 1.6353041049910755
- 1.6361657061227939
- 1.6318416682685293
- 1.6366246793328263
- 1.6315315554781658
- 1.6320794413729411
- 1.6403414330831387
- 1.6309683177529313
- 1.6308011746988065
- 1.6331282069043416
- 1.631965730248428
- 1.6346203030609503
- 1.633761786833042
- 1.6370926455753605
- 1.6374629009060744
- 1.6396760766099139
- 1.6292356107293107
- 1.6313314176187284
- 1.6414064314307235
- 1.6350836346789104
virtual_batch_multiplier: 0
vocab_size: 10000
