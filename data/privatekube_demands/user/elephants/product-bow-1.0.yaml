accuracy: 0.6057763646456821
adaptive_batch_size: 1
alphas:
- 1.5
- 1.75
- 2
- 2.5
- 3
- 4
- 5
- 6
- 8
- 16
- 32
- 64
batch_size: 323
best_alpha: 64.0
delta: 1.0e-09
device: cuda
dp: 1
dp_eval: 0
dropout: 0.25
dynamic_clipping: 0
embedding_dim: 100
epsilon: 0.9941723967623615
hidden_dim: 100
hidden_dim_1: 240
hidden_dim_2: 195
learning_rate: 0.01
learning_rate_scheduler: 1
loss: 1.2137522589076648
max_grad_norm: 1.0
max_text_len: 75
model: bow
n_blocks: 5000
n_blocks_test: 200
n_epochs: 60
n_train_users: 104866
n_trainable_parameters: 1111
n_workers: 6
noise: 3.0922445678710937
non_dp_batch_size: 256
per_layer_clipping: 0
rdp_epsilons:
- 0.015246720709001238
- 0.017789400585570547
- 0.0203325266596955
- 0.025420117700272048
- 0.030509495073951044
- 0.0406936139769089
- 0.05088489372168824
- 0.06108334468557398
- 0.0815018019011586
- 0.16346457177153687
- 0.32879474723512436
- 0.6652316691917836
target_epsilon: 1.0
task: product
test_size: 39077
timeframe_days: 0
total_time: 126.09072303771973
train_size: 1332804
training_accuracy_epochs:
- 0.4631349491760319
- 0.5668118942850902
- 0.5844799741918658
- 0.5945801162793313
- 0.5999885146264676
- 0.6054255863030752
- 0.6107670966489815
- 0.6139586262496901
- 0.6155735031690126
- 0.6190039191716983
- 0.6196250251413863
- 0.6217272318439719
- 0.6229216669812615
- 0.6243263221817252
- 0.6260749747355779
- 0.6268489684587644
- 0.6272216330707809
- 0.6281294039002171
- 0.6289607304849743
- 0.6296391674765834
- 0.6304991620558279
- 0.6310247144213429
- 0.6305469381220546
- 0.6328402540566008
- 0.6331078077172055
- 0.6335378042709681
- 0.6334709151659483
- 0.6345984604623582
- 0.6352004574404823
- 0.6356877868558154
- 0.6362611156554869
- 0.6349233490081481
- 0.636031785312994
- 0.6359648951041845
- 0.6363184475972329
- 0.6366911127611443
- 0.6370351082748837
- 0.6376562149804316
- 0.6385544304862435
- 0.6394430903005012
- 0.6398539773108046
- 0.6398921969496174
- 0.6399399755913534
- 0.6392806475913083
- 0.6410866324548368
- 0.639863532450464
- 0.6396724206429941
- 0.6410866324548368
- 0.6411439658683024
- 0.6416312956515654
- 0.6409432998409977
- 0.6412012989138379
- 0.6410101880261927
- 0.6417555152266113
- 0.6416886255696967
- 0.641736407522802
- 0.6414879624858315
- 0.6408573019283789
- 0.6418606263619883
- 0.642023069255146
training_loss_epochs:
- 1.716916056326878
- 1.4937509651537295
- 1.4160961321106664
- 1.3667361048268682
- 1.3392332361804113
- 1.3178086457429108
- 1.297870488078506
- 1.2826456434932756
- 1.2707101187588257
- 1.2579950752817555
- 1.246671027607388
- 1.2387904189986947
- 1.2276462620055233
- 1.2207764289629313
- 1.2129986536355666
- 1.2063501937153898
- 1.2028662238591983
- 1.1983558684587479
- 1.1927108407756428
- 1.192841257210131
- 1.1854326439860425
- 1.1834199207800407
- 1.1776303954330491
- 1.1733965205925483
- 1.1716714976010498
- 1.1686103051459347
- 1.1653879720855642
- 1.1625446609140915
- 1.1586300749101757
- 1.1557522662627844
- 1.1527094359015242
- 1.1533973040404144
- 1.150383017681263
- 1.1508985310792923
- 1.149289952568066
- 1.1491496987916805
- 1.1467146106340267
- 1.1421574398323342
- 1.1399938857849734
- 1.1374623368919632
- 1.1332340954262534
- 1.1309723682977535
- 1.1310297302257868
- 1.1296795882560589
- 1.1263770209050472
- 1.1275969000510226
- 1.1269535139019107
- 1.1255339222189822
- 1.12680797610018
- 1.1269808849434795
- 1.1285087747706308
- 1.1274941019070002
- 1.1261027528915877
- 1.125647383707541
- 1.1258601308972747
- 1.1247787510539278
- 1.1263510115720607
- 1.1269822595296082
- 1.1248810572756662
- 1.1245187397724317
training_time: 84.7939989566803
user_level: 1
validation_accuracy_epochs:
- 0.514250306821451
- 0.5476656032771599
- 0.561573986600085
- 0.5650945349437434
- 0.5739472086836652
- 0.5734905716849537
- 0.580620717711565
- 0.5836162436299208
- 0.5810053988200862
- 0.5886178263803807
- 0.5859856024021055
- 0.5912714135356065
- 0.591304187367602
- 0.5945184129040416
- 0.5899770303470332
- 0.5902719541293818
- 0.5960863421602947
- 0.595413863658905
- 0.5964026436573122
- 0.5950291781890683
- 0.6006925513104695
- 0.5965971219830397
- 0.598186426046418
- 0.5978302359580994
- 0.6027384938263312
- 0.5991032443395475
- 0.6021343978439889
- 0.6018900565984773
- 0.5980425244424401
- 0.598887398475554
- 0.6003149965914284
- 0.5993440325667219
- 0.599930316936679
- 0.5989016468931989
- 0.6042558463608346
- 0.6023032287272011
- 0.6026665393899127
- 0.6029650278207732
- 0.6024549639806515
- 0.6014839970484013
- 0.603335456150334
- 0.6045472054946713
- 0.6042344730074812
- 0.6066829009753901
- 0.6058850491919169
- 0.6071003515545915
- 0.6053885206943606
- 0.6096278588946272
- 0.6056870018563619
- 0.6044289535138665
- 0.603475801828431
- 0.601412049153956
- 0.605766079774717
- 0.6060716858724269
- 0.6059135431196632
- 0.6061471962347263
- 0.6065967097515013
- 0.6074308942003948
- 0.606122266955492
- 0.6055074901115604
validation_loss_epochs:
- 1.6384902727313158
- 1.5323737685273333
- 1.4701613682072336
- 1.4370026559364506
- 1.4090861314680518
- 1.3945182212969152
- 1.368759620480421
- 1.364074590729504
- 1.3521995253679229
- 1.3259416469713536
- 1.3356374153276769
- 1.3191356949689912
- 1.3108370304107666
- 1.3006473954130964
- 1.3026954982338883
- 1.2916819089796485
- 1.2880345495735728
- 1.2814391415293624
- 1.2789171905052372
- 1.2706069364780332
- 1.2654194279414852
- 1.270006842729522
- 1.2600012959503546
- 1.2563659417919997
- 1.2475750359093272
- 1.2435322331219185
- 1.2450767871810169
- 1.2407752537145846
- 1.2392841751982526
- 1.233066044202665
- 1.2352380781638912
- 1.23085500845095
- 1.2295983273808548
- 1.2315763409544782
- 1.2320012581057664
- 1.2221718823037497
- 1.2189624222313487
- 1.2239559830688849
- 1.2249309086218112
- 1.2224992193826816
- 1.2150966946671649
- 1.2196088796708642
- 1.2039354748842193
- 1.2014019140383092
- 1.2078152633294827
- 1.2116777577051303
- 1.2050958842765995
- 1.2021334432974093
- 1.2128350502107201
- 1.2176155927704602
- 1.2160857741425677
- 1.210125777779556
- 1.206427952138389
- 1.207818048756297
- 1.205792017099334
- 1.2051896670969522
- 1.206659700812363
- 1.205898296542284
- 1.209884448749263
- 1.2077958612907223
virtual_batch_multiplier: 0
vocab_size: 10000
