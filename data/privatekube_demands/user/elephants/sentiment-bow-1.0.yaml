accuracy: 0.7052316711873424
adaptive_batch_size: 1
alphas:
- 1.5
- 1.75
- 2
- 2.5
- 3
- 4
- 5
- 6
- 8
- 16
- 32
- 64
batch_size: 160
best_alpha: 64.0
delta: 1.0e-09
device: cuda
dp: 1
dp_eval: 0
dropout: 0.25
dynamic_clipping: 0
embedding_dim: 100
epsilon: 0.9928036901070076
hidden_dim: 100
hidden_dim_1: 240
hidden_dim_2: 195
learning_rate: 0.01
learning_rate_scheduler: 1
loss: 0.5806333776639432
max_grad_norm: 1.0
max_text_len: 50
model: bow
n_blocks: 500
n_blocks_test: 200
n_epochs: 60
n_train_users: 25671
n_trainable_parameters: 101
n_workers: 6
noise: 4.343452758789063
non_dp_batch_size: 256
per_layer_clipping: 0
rdp_epsilons:
- 0.015223089230194336
- 0.017761794204321918
- 0.020300934782401924
- 0.02538052362979215
- 0.03046185698337953
- 0.040629761994322666
- 0.05080465954404721
- 0.06098655938290758
- 0.08137140503708493
- 0.16319284422284
- 0.32820621989402915
- 0.6638629625364297
target_epsilon: 1.0
task: sentiment
test_size: 39077
timeframe_days: 0
total_time: 75.66910076141357
train_size: 165215
training_accuracy_epochs:
- 0.6494921989738941
- 0.6487890742719173
- 0.6506640754640103
- 0.649609387293458
- 0.6513281352818012
- 0.6493750128895044
- 0.6505078230053186
- 0.6536328256130218
- 0.6579687617719173
- 0.659843759983778
- 0.6633984487503767
- 0.6665625136345625
- 0.667265635728836
- 0.6725781351327896
- 0.6758984491229058
- 0.6800781335681677
- 0.6851953268051147
- 0.6868359457701445
- 0.6867187630385161
- 0.6918359458446502
- 0.6903906363993884
- 0.6929687608033419
- 0.6929296977818012
- 0.695507825165987
- 0.6951953239738942
- 0.6960156373679638
- 0.6984765708446503
- 0.7003515750169754
- 0.7019922003149986
- 0.702148450165987
- 0.7034765731543302
- 0.7047656361013651
- 0.7037500135600567
- 0.7028515715152025
- 0.7050390753895044
- 0.7060937605798244
- 0.7068750109523535
- 0.7079687647521495
- 0.7074609518051147
- 0.7087890777736903
- 0.7060937643051147
- 0.7078125115484
- 0.7085937630385161
- 0.7100781381130219
- 0.7077343881130218
- 0.7065625131130219
- 0.7103515736758709
- 0.7093359500169754
- 0.7074218887835741
- 0.7084765736013651
- 0.7075000107288361
- 0.7091015737503767
- 0.708984388038516
- 0.7090234521776437
- 0.7101172007620334
- 0.7107421968132257
- 0.7095703270286322
- 0.7101171985268593
- 0.7087500121444463
- 0.7102734498679638
training_loss_epochs:
- 0.6884823773056269
- 0.6691636841744184
- 0.6594974096864462
- 0.6521735366433858
- 0.6409522738307715
- 0.6367966268211603
- 0.6291441444307566
- 0.6206925731152296
- 0.6149429112672806
- 0.6108709760010242
- 0.604217536188662
- 0.6036549488082528
- 0.6029297273606062
- 0.5973663687705993
- 0.5953427299857139
- 0.5916799809783697
- 0.5876544490456581
- 0.5878966392949223
- 0.5879918687045574
- 0.5844497619196772
- 0.5856944940984249
- 0.5872263381257653
- 0.5849255207926035
- 0.5810589486733079
- 0.5840427057817579
- 0.5816311126574873
- 0.5789581267163157
- 0.5787865931168199
- 0.5758053701370954
- 0.5749381432309747
- 0.5732975313439965
- 0.5721713893115521
- 0.5748532056808472
- 0.5741834381595254
- 0.570710182376206
- 0.5698060102760791
- 0.5699121192097664
- 0.5706293862313032
- 0.5718474140390754
- 0.5729700133204461
- 0.5744419062510133
- 0.570161122828722
- 0.567784345895052
- 0.5681531602516771
- 0.5689424211159348
- 0.5689303334802389
- 0.5666066305711865
- 0.5689731035381556
- 0.5706636065617203
- 0.5687682501971721
- 0.5678114870563149
- 0.5683402296155691
- 0.568689801543951
- 0.5684491712599993
- 0.5684640940278769
- 0.5666080409660935
- 0.568797237239778
- 0.5652865124866366
- 0.5675173541530967
- 0.5673510227352381
training_time: 51.27234983444214
user_level: 1
validation_accuracy_epochs:
- 0.6526122609289681
- 0.6540234772170462
- 0.6532635914116371
- 0.653914922621192
- 0.6537416948050987
- 0.6539819051579732
- 0.6547417866020668
- 0.6549173280960177
- 0.6631374867950998
- 0.6728196740150452
- 0.6735911064031648
- 0.6693228148832554
- 0.6698771366258947
- 0.669595355667719
- 0.6704129831093114
- 0.6785130384491711
- 0.6823771377889122
- 0.6827813314228524
- 0.6851857101045004
- 0.6872782787171806
- 0.6880312387536212
- 0.6890197825141069
- 0.6891375743761295
- 0.6871142990705443
- 0.6894563116678377
- 0.6892599899594377
- 0.6942373572326288
- 0.6984271218137044
- 0.6935398338771448
- 0.6970759550245796
- 0.6952166600925166
- 0.6980783539574321
- 0.6945630224739633
- 0.6954014366719781
- 0.6980575664741236
- 0.6985703205190054
- 0.7061460803194743
- 0.6969396813613612
- 0.7063423983934449
- 0.6976764718206917
- 0.7007137028182425
- 0.6996166051887884
- 0.7007460397918049
- 0.7011594714187994
- 0.7022080712202119
- 0.7006166933513269
- 0.6992124108279624
- 0.7006397937856069
- 0.7010093447638721
- 0.7017900158719319
- 0.7015867618525901
- 0.7013904394173041
- 0.7013581031706275
- 0.7025198776547502
- 0.7017599860342537
- 0.7019770988603917
- 0.7018685428107657
- 0.7016514350728291
- 0.7019771017679354
- 0.7019771017679354
validation_loss_epochs:
- 0.6981091121347939
- 0.6627591848373413
- 0.6489319917632312
- 0.650750656680363
- 0.633169808765737
- 0.6335820272201445
- 0.6424878915635551
- 0.6467085131784764
- 0.6189613160563678
- 0.6054763859365044
- 0.6030405069269785
- 0.6128953920631874
- 0.6152309608168718
- 0.6187781037353888
- 0.6170068434098872
- 0.6031293185745797
- 0.5985926331543341
- 0.6028757756803094
- 0.6003041223781865
- 0.5966605202453893
- 0.5971150638126745
- 0.5984379954454375
- 0.5989108136514338
- 0.6037918245647012
- 0.6017406044936762
- 0.5997275484771263
- 0.59083480006311
- 0.5848081497157492
- 0.5903142919627632
- 0.5909472972881503
- 0.5942315550112143
- 0.588132686004406
- 0.5920082581479374
- 0.589729385405052
- 0.5850644667701024
- 0.5869963656111461
- 0.58214394021325
- 0.5943499952554703
- 0.5822330527916187
- 0.5977552911130394
- 0.588028791474133
- 0.5883335369389232
- 0.5860974974021679
- 0.5874213794382607
- 0.5855388456001515
- 0.5861959504645046
- 0.589822720463683
- 0.5872380413660189
- 0.5858171691254872
- 0.5866442017438935
- 0.5862089235608171
- 0.5858453134211098
- 0.5860481422121931
- 0.5844902915925514
- 0.5854174778228853
- 0.5849306230864874
- 0.5851791206656433
- 0.5851226877875444
- 0.5850430712467287
- 0.5851383154712072
virtual_batch_multiplier: 0
vocab_size: 10000
