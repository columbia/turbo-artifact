accuracy: 0.6020697863734499
adaptive_batch_size: 1
alphas:
- 1.5
- 1.75
- 2
- 2.5
- 3
- 4
- 5
- 6
- 8
- 16
- 32
- 64
batch_size: 160
best_alpha: 8.0
delta: 1.0e-09
device: cuda
dp: 1
dp_eval: 0
dropout: 0.25
dynamic_clipping: 0
embedding_dim: 100
epsilon: 4.9741828175039
hidden_dim: 40
hidden_dim_1: 240
hidden_dim_2: 195
learning_rate: 0.01
learning_rate_scheduler: 1
loss: 1.4490940312949978
max_grad_norm: 1.0
max_text_len: 30
model: lstm
n_blocks: 500
n_blocks_test: 200
n_epochs: 60
n_train_users: 25671
n_trainable_parameters: 23171
n_workers: 6
noise: 1.108621826171875
non_dp_batch_size: 256
per_layer_clipping: 0
rdp_epsilons:
- 0.34943532717572356
- 0.40876894910193684
- 0.4684274397284847
- 0.5887387098351449
- 0.710410140098067
- 0.9580119678948786
- 1.211624823173879
- 1.4717102432329252
- 2.013716269368699
- 10497.769936929084
- 74654.57022462894
- 200428.6641576459
target_epsilon: 5.0
task: product
test_size: 39077
timeframe_days: 0
total_time: 453.77929520606995
train_size: 165215
training_accuracy_epochs:
- 0.4527343833819032
- 0.5444140749052166
- 0.569375011511147
- 0.5928906356915832
- 0.5989843865856528
- 0.5973437622189521
- 0.6041797012090683
- 0.6054296988993884
- 0.6052734514698386
- 0.6088281366974115
- 0.6133984491229058
- 0.615625013411045
- 0.6132421987131238
- 0.6145703226327897
- 0.6147656364366412
- 0.6158984500914813
- 0.6123828267678618
- 0.6147265732288361
- 0.6162109496071935
- 0.6175781382247806
- 0.6196093864738941
- 0.6214453250169754
- 0.6208593862131238
- 0.6262890737503767
- 0.6278906367719174
- 0.6285156372934579
- 0.6273828256875277
- 0.6313671983778477
- 0.6351953227072954
- 0.6341406386345625
- 0.6334375113248825
- 0.6345703266561031
- 0.6360937617719173
- 0.6340625114738941
- 0.6358593873679638
- 0.6362109497189522
- 0.6364453218877315
- 0.6363281361758709
- 0.6360937617719173
- 0.636523449420929
- 0.6366406332701444
- 0.6357812613248826
- 0.6344140749424696
- 0.6350000105798245
- 0.6358203239738941
- 0.6362109489738941
- 0.6372265730053186
- 0.6350781373679638
- 0.6372656378895044
- 0.638164072856307
- 0.6320312630385161
- 0.6373437602072954
- 0.6363671991974116
- 0.6353515744209289
- 0.638007826730609
- 0.6344140749424696
- 0.6363671973347664
- 0.6364453222602606
- 0.6348046995699406
- 0.6360937640070915
training_loss_epochs:
- 1.803965237736702
- 1.625578272342682
- 1.59126403555274
- 1.5340287134051322
- 1.5218580842018128
- 1.5369739897549153
- 1.5269490487873554
- 1.506656240671873
- 1.4898814663290978
- 1.4865503318607807
- 1.464750975370407
- 1.4481594935059547
- 1.4627989955246448
- 1.4526163149625062
- 1.4428005889058113
- 1.442098205909133
- 1.4368193969130516
- 1.432486753165722
- 1.4143776960670948
- 1.4085819385945797
- 1.3807551790028811
- 1.3910437248647214
- 1.3723154664039612
- 1.3944605723023415
- 1.3940670199692249
- 1.39534427896142
- 1.3967862494289875
- 1.3273225110024214
- 1.3512982543557883
- 1.3569203849881888
- 1.3499781861901283
- 1.354122693091631
- 1.348863447830081
- 1.3478249575942756
- 1.3437457118183374
- 1.3418110288679599
- 1.3388815522193909
- 1.3463100396096706
- 1.352508095651865
- 1.3406953934580088
- 1.3461241878569126
- 1.3440474659204482
- 1.3486809678375722
- 1.3442694962024688
- 1.3509076599031686
- 1.3478675369173287
- 1.3375539466738702
- 1.349757868051529
- 1.3395700134336948
- 1.3350104469805957
- 1.3504766255617142
- 1.3428948625922204
- 1.3451902698725462
- 1.3537704706192017
- 1.3433315485715867
- 1.3475081987679005
- 1.3434737913310528
- 1.3372088350355624
- 1.3558784216642379
- 1.3472707353532314
training_time: 426.7002329826355
user_level: 1
validation_accuracy_epochs:
- 0.5052799440738631
- 0.5422926039957419
- 0.5427245123357307
- 0.5487804954371801
- 0.5682465016115003
- 0.5706554985627895
- 0.5506628875325366
- 0.5691726840123897
- 0.5773166247257372
- 0.574083066931585
- 0.5767184147020665
- 0.5796170656274005
- 0.5818713151100205
- 0.5712975881448606
- 0.5814763601960206
- 0.5910777122509189
- 0.582499549156282
- 0.5857677499695522
- 0.5922441010068102
- 0.5909922500935997
- 0.5871189142145762
- 0.5882298717411553
- 0.5921817356493415
- 0.587021909472419
- 0.5966994784227232
- 0.5943343577588477
- 0.5992008625734143
- 0.6025337336993799
- 0.6012841964640269
- 0.6037855817050468
- 0.6009977939652233
- 0.6033513647754017
- 0.604102009680213
- 0.6036354535963477
- 0.603526894639178
- 0.6046147557293496
- 0.6047233106886468
- 0.6044738670674766
- 0.6053746444423024
- 0.6050166429542914
- 0.605450864245252
- 0.6045062033141532
- 0.6040719812963067
- 0.60494042169757
- 0.6045062025872673
- 0.6042890926686729
- 0.6049404187900264
- 0.60494042169757
- 0.6047233154134053
- 0.6052660876657905
- 0.6041805387997046
- 0.6046147564562355
- 0.604723309598318
- 0.6043976472645272
- 0.6047233132327475
- 0.6049404209706841
- 0.6057003067760933
- 0.6047233154134053
- 0.6047233125058616
- 0.6041805373459328
validation_loss_epochs:
- 1.6757888270587455
- 1.740528808861244
- 1.5962190337297393
- 1.5352539056684913
- 1.6692583982537432
- 1.7739827109546196
- 1.806304190216995
- 1.6062695689317656
- 1.6637295658995466
- 1.5801698129351547
- 1.511565667826955
- 1.5376827556912491
- 1.545901192397606
- 1.546608055510172
- 1.5302748432973536
- 1.5170817026277867
- 1.5057927500910875
- 1.48920880585182
- 1.4875177115928837
- 1.5024284226138418
- 1.459734271212322
- 1.507383695462855
- 1.4849223160162204
- 1.5587538916890213
- 1.5460815327923472
- 1.516468709561883
- 1.4228022738200863
- 1.4532740886618452
- 1.4629425406455994
- 1.4641947818965446
- 1.4645467734918363
- 1.4563019624570521
- 1.4547243249125597
- 1.4575130983096798
- 1.4589228441075581
- 1.4561683215746066
- 1.4564943546202125
- 1.4553117650311167
- 1.4519417714781877
- 1.4534475861526117
- 1.4529944178534717
- 1.4565643580948435
- 1.4589415032689164
- 1.4537888838023674
- 1.4578754189537793
- 1.4564777656299313
- 1.454575388896756
- 1.4550772963500604
- 1.4556292135541031
- 1.4548815038146041
- 1.456744102443137
- 1.4546784714954655
- 1.4562451955748767
- 1.4559498211232627
- 1.457136350434001
- 1.4539874588570945
- 1.4541874019111074
- 1.4580679288724574
- 1.457451459838123
- 1.4580079171715714
virtual_batch_multiplier: 0
vocab_size: 10000
