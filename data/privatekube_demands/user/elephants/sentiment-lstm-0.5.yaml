accuracy: 0.7586754301319951
adaptive_batch_size: 1
alphas:
- 1.5
- 1.75
- 2
- 2.5
- 3
- 4
- 5
- 6
- 8
- 16
- 32
- 64
batch_size: 428
best_alpha: 64.0
delta: 1.0e-09
device: cuda
dp: 1
dp_eval: 0
dropout: 0.25
dynamic_clipping: 0
embedding_dim: 100
epsilon: 0.4982638182080895
hidden_dim: 40
hidden_dim_1: 240
hidden_dim_2: 195
learning_rate: 0.01
learning_rate_scheduler: 1
loss: 0.6665733482526697
max_grad_norm: 1.0
max_text_len: 50
model: lstm
n_blocks: 10000
n_blocks_test: 200
n_epochs: 60
n_train_users: 183808
n_trainable_parameters: 22761
n_workers: 6
noise: 5.197936401367188
non_dp_batch_size: 256
per_layer_clipping: 0
rdp_epsilons:
- 0.003946472215261565
- 0.004604319639156529
- 0.0052621963978546635
- 0.006578037204018518
- 0.007893994701642227
- 0.0105262599629736
- 0.01315899234976433
- 0.015792192029353693
- 0.021059993937148247
- 0.04214992683334204
- 0.08441995423318889
- 0.16932309063751158
target_epsilon: 0.5
task: sentiment
test_size: 39077
timeframe_days: 0
total_time: 1673.3642354011536
train_size: 2545758
training_accuracy_epochs:
- 0.6497505407233338
- 0.6584591219475219
- 0.7115874541945113
- 0.724522338741587
- 0.7347014115684793
- 0.7379637254979505
- 0.7403274040955764
- 0.7437040901684261
- 0.7440962220405365
- 0.7451255657734015
- 0.7456647463333912
- 0.7475164786919014
- 0.7495098112624287
- 0.7505990637765898
- 0.7508332547330079
- 0.7520586638461738
- 0.7535727265831473
- 0.7543569882432898
- 0.7557076629780946
- 0.7569221815307101
- 0.757439575967811
- 0.7578534924344861
- 0.7585941851833762
- 0.7596779917225693
- 0.7602988670478056
- 0.7607835838288972
- 0.7610831289858251
- 0.7596344232281327
- 0.7604077927994005
- 0.7602444047972317
- 0.7608489395577313
- 0.7604404697607169
- 0.760767245348239
- 0.7618564980013387
- 0.7607345686647997
- 0.7611539294391801
- 0.7608925098583693
- 0.7607454617262442
- 0.7609796500428295
- 0.761649542199426
- 0.7609469723868203
- 0.7609469729425746
- 0.761083127735378
- 0.7619490859669683
- 0.7608108159267541
- 0.7610667900883512
- 0.7611811623706685
- 0.7606909979473461
- 0.7606147498517604
- 0.7609306329335922
- 0.7611266987307088
- 0.760897956250153
- 0.7605984126215493
- 0.7615024922610997
- 0.7617203429306582
- 0.7605493950954962
- 0.7611375927647233
- 0.7609796497649524
- 0.7611539299949344
- 0.7614099042954701
training_loss_epochs:
- 0.7070671526980011
- 0.7664909391970067
- 0.7482745562677895
- 0.7376559503150709
- 0.7040335513772942
- 0.6980053228098195
- 0.7080001742134006
- 0.7068461245034402
- 0.7026255761548912
- 0.7016765357175351
- 0.6916012663941283
- 0.6883770986036821
- 0.6864712185515113
- 0.68262556591234
- 0.6820314797488126
- 0.6825179081140976
- 0.6785242500560823
- 0.6786987031256402
- 0.6765807775350717
- 0.6731072757349703
- 0.6750135984454122
- 0.6706093381612729
- 0.6733169668204301
- 0.6661494846666332
- 0.6690676296368624
- 0.669406287736826
- 0.6676532891524699
- 0.6714334170440416
- 0.667645536102615
- 0.6675104037587182
- 0.6666173462545399
- 0.6676000844348561
- 0.6662657135850066
- 0.6629532181716465
- 0.6662683755784602
- 0.6664004851054478
- 0.6658259935868092
- 0.666932751387705
- 0.6658069994344022
- 0.6647084488735332
- 0.667595393660463
- 0.666841232276463
- 0.6660542378336678
- 0.6644770482898036
- 0.6662987220537412
- 0.6668893193309402
- 0.6657394498656124
- 0.6680941234379659
- 0.6677889366805693
- 0.6656764838801119
- 0.6658794727080908
- 0.6667551633083459
- 0.6679115074497837
- 0.6648684508039123
- 0.6647576765441672
- 0.6670028410313569
- 0.6661010299807106
- 0.6669004719296258
- 0.666169376640053
- 0.6659582982530127
training_time: 1612.1672368049622
user_level: 1
validation_accuracy_epochs:
- 0.6531374473725596
- 0.6893755043706586
- 0.7033496152970099
- 0.7207980194399434
- 0.7302864463098587
- 0.7395341742423273
- 0.7389847117085611
- 0.7309120912705699
- 0.7418487398855148
- 0.7326391012437882
- 0.7425197728218571
- 0.7417279904888522
- 0.745421088510944
- 0.7456244999362577
- 0.7450134388862117
- 0.7492876091311055
- 0.7496271787151214
- 0.7501239718929413
- 0.7520017239355272
- 0.7535463879185338
- 0.7549703005821474
- 0.756690013793207
- 0.7541720309565144
- 0.7550310838607049
- 0.753975106823829
- 0.755385241200847
- 0.7556486206669961
- 0.7561843106823583
- 0.7560708503569326
- 0.755535162264301
- 0.7563196497578775
- 0.7566964953176437
- 0.7577070959152714
- 0.757729793748548
- 0.756726486067618
- 0.7568999105884183
- 0.7557385833032669
- 0.7564549869106661
- 0.7572394801724341
- 0.7575109754839251
- 0.7568018513341104
- 0.7560173638405339
- 0.7577824631044942
- 0.7577824669499551
- 0.7581747135808391
- 0.7573902164736102
- 0.756703790157072
- 0.7576844019274558
- 0.7564096085486873
- 0.7571941018104553
- 0.7576844057729167
- 0.758272772835147
- 0.7572921649102242
- 0.7568999182793402
- 0.7574882814961095
- 0.756311547371649
- 0.7571940998877248
- 0.7568999163566097
- 0.7571940941195334
- 0.7573902164736102
validation_loss_epochs:
- 0.7210325329534469
- 0.8102461272670377
- 0.77034334405776
- 0.7343657420527551
- 0.6696685898688531
- 0.6919498962740744
- 0.7187984374261671
- 0.7468773664966706
- 0.6849462870628603
- 0.7095868702857725
- 0.6927247874198421
- 0.6986412925104941
- 0.687603606331733
- 0.6947553830762063
- 0.6988724604729684
- 0.6827254949077484
- 0.6831054322181209
- 0.6876204052279072
- 0.6807686398106236
- 0.6756518137070441
- 0.667507485035927
- 0.6673362005141473
- 0.6793025078312043
- 0.672147274017334
- 0.679315451652773
- 0.6786661532617384
- 0.6684841878952519
- 0.6710591143177401
- 0.6728092547385923
- 0.6736108237697233
- 0.6727936633171574
- 0.6695192783109604
- 0.6682484496024347
- 0.6684974585810015
- 0.6714420068648553
- 0.671120043723814
- 0.6742548096564508
- 0.6719864914494176
- 0.6713309288024902
- 0.6702499351193828
- 0.6719472331385459
- 0.672693173731527
- 0.6703436163163954
- 0.6698251789616
- 0.6683388294712189
- 0.6701890999271024
- 0.6719629841466104
- 0.6697482774334569
- 0.6735740823130454
- 0.671054930456223
- 0.669948531735328
- 0.6672051222093643
- 0.6696977115446522
- 0.6708905677641591
- 0.6704388126250236
- 0.6735397942604557
- 0.67076370023912
- 0.6712051514656313
- 0.6709319468467466
- 0.6716005629108798
virtual_batch_multiplier: 0
vocab_size: 10000
