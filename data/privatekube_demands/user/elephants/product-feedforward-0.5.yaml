accuracy: 0.6108675003051758
adaptive_batch_size: 1
alphas:
- 1.5
- 1.75
- 2
- 2.5
- 3
- 4
- 5
- 6
- 8
- 16
- 32
- 64
batch_size: 428
best_alpha: 64.0
delta: 1.0e-09
device: cuda
dp: 1
dp_eval: 0
dropout: 0.25
dynamic_clipping: 0
embedding_dim: 100
epsilon: 0.4982638182080895
hidden_dim: 100
hidden_dim_1: 185
hidden_dim_2: 150
learning_rate: 0.01
learning_rate_scheduler: 1
loss: 1.6812169759169868
max_grad_norm: 1.0
max_text_len: 60
model: feedforward
n_blocks: 10000
n_blocks_test: 200
n_epochs: 60
n_train_users: 183808
n_trainable_parameters: 48246
n_workers: 6
noise: 5.197936401367188
non_dp_batch_size: 256
per_layer_clipping: 0
rdp_epsilons:
- 0.003946472215261565
- 0.004604319639156529
- 0.0052621963978546635
- 0.006578037204018518
- 0.007893994701642227
- 0.0105262599629736
- 0.01315899234976433
- 0.015792192029353693
- 0.021059993937148247
- 0.04214992683334204
- 0.08441995423318889
- 0.16932309063751158
target_epsilon: 0.5
task: product
test_size: 39077
timeframe_days: 0
total_time: 218.53865432739258
train_size: 2545758
training_accuracy_epochs:
- 0.5412173326211897
- 0.6129882398185197
- 0.6239188969274223
- 0.6242729048906784
- 0.6235104282419165
- 0.6223993890491121
- 0.6308084242827409
- 0.634816879159087
- 0.6372622516883281
- 0.6383787372729162
- 0.6407206325264244
- 0.6412707043416572
- 0.6412761505945024
- 0.6420821982385951
- 0.6416791719156545
- 0.6422782626463261
- 0.6420495207215244
- 0.642234692067811
- 0.6417390829199677
- 0.641733636528184
- 0.6417717602980998
- 0.6415702488038924
- 0.642408973270363
- 0.642049519887893
- 0.6425669150196867
- 0.6417826519701586
- 0.6413578425809776
- 0.6425941478122364
- 0.6423054941884287
- 0.6417064055418357
- 0.6424852202544401
- 0.642196568992588
- 0.6420876440746245
- 0.6424362052292812
- 0.6416301566126186
- 0.6415266782253772
- 0.6413796305100679
- 0.6425614698783501
- 0.6415048932139968
- 0.6418534540907764
- 0.6420658585074899
- 0.6418915793890164
- 0.6418262237991208
- 0.6425560240423207
- 0.6419460394165732
- 0.6424961147052702
- 0.6418044373983548
- 0.6420604115599519
- 0.6416410498130016
- 0.6417989900340011
- 0.6419079175917974
- 0.6428719074020297
- 0.6425778075253769
- 0.6420440737739865
- 0.6422455863797025
- 0.6423817427008303
- 0.6425995928146344
- 0.6419841651316289
- 0.641886131607847
- 0.6419896116623511
training_loss_epochs:
- 1.5970903407165777
- 1.4656390279600948
- 1.5212022041107391
- 1.6452138423919678
- 1.7324210413686045
- 1.8150896067385907
- 1.7269110187783943
- 1.6733400980075757
- 1.6313041162379693
- 1.5986456023507463
- 1.5793544101270485
- 1.5709972845646607
- 1.5742473135461341
- 1.5634957679779657
- 1.5649085075427325
- 1.5638464411377628
- 1.563401924424516
- 1.5614290593109486
- 1.562395755227629
- 1.5661583891559592
- 1.5617188209698194
- 1.562213497006254
- 1.5614938610917204
- 1.5611794372816463
- 1.5597935598213355
- 1.5637143194536507
- 1.5640677258129165
- 1.5605587739766618
- 1.5636603873926442
- 1.5643357591473417
- 1.5597720349029505
- 1.5636762858548643
- 1.5614464958230931
- 1.5618637172214358
- 1.5609585569177196
- 1.561407543840386
- 1.561570585190833
- 1.5585032757067736
- 1.564340502787859
- 1.5598822301362223
- 1.5634155187295589
- 1.5624611575008829
- 1.5668979571971582
- 1.560558553342219
- 1.5647184231898168
- 1.560371843251315
- 1.5631041718529655
- 1.5630988107694612
- 1.5642005380217012
- 1.5623891898246356
- 1.5659376229995337
- 1.5571745225599596
- 1.5609881391614189
- 1.5631702396419498
- 1.5588046395695292
- 1.5636660924880377
- 1.561260982271119
- 1.5624886006464214
- 1.561828994528675
- 1.5627848974196783
training_time: 159.32916355133057
user_level: 1
validation_accuracy_epochs:
- 0.5674936059982546
- 0.5734032046410346
- 0.594657346125572
- 0.5894187804191343
- 0.5928938677234035
- 0.5897664523893787
- 0.6001739021270506
- 0.6064384733476946
- 0.6085860883035967
- 0.6095051034804313
- 0.6089694211559911
- 0.6121965204515765
- 0.6113674544518993
- 0.6126795295746096
- 0.6125887632369995
- 0.6114501203260114
- 0.6118196729690798
- 0.6121511324759452
- 0.6119550139673294
- 0.6126414422065981
- 0.6123472548300221
- 0.6123699526632985
- 0.611706220334576
- 0.6130563809025672
- 0.6120004000202302
- 0.6127848952047287
- 0.6122945816286148
- 0.6127848855910762
- 0.6100391764794627
- 0.6126868282594988
- 0.6127848875138068
- 0.6118042661297706
- 0.6121965146833851
- 0.6124907020599611
- 0.6113139660127701
- 0.6128829506135756
- 0.6116081553120767
- 0.6114120271898085
- 0.611019786327116
- 0.6126868224913075
- 0.6140596770471142
- 0.6118042776661534
- 0.611804275743423
- 0.6111178475041543
- 0.6116081476211548
- 0.6127848875138068
- 0.6122945816286148
- 0.611804275743423
- 0.6119023369204614
- 0.6125887632369995
- 0.6118042699752315
- 0.6116081514666157
- 0.6123926428056532
- 0.6142558051693824
- 0.6122945777831539
- 0.6125887593915386
- 0.6116081514666157
- 0.6116081533893463
- 0.6118042738206925
- 0.61258876515973
validation_loss_epochs:
- 1.4989263434563913
- 1.6458253898928243
- 1.7577527376913256
- 1.808680611272012
- 1.9160340678307317
- 1.9037598679142613
- 1.8159877407935359
- 1.786815797128985
- 1.733345300920548
- 1.7067312694364978
- 1.7017792963212537
- 1.6842767500108289
- 1.6861831026692544
- 1.6780511909915554
- 1.6756499467357513
- 1.6828191434183428
- 1.6822755490579913
- 1.6811493981269099
- 1.6836984695926789
- 1.6827424264723254
- 1.6814638683872838
- 1.679416302711733
- 1.6836666061032204
- 1.6741025101753972
- 1.6796792476407942
- 1.678994817118491
- 1.682358653314652
- 1.6772922508178219
- 1.6901247116827196
- 1.681546603479693
- 1.6789767319156277
- 1.6828754986486127
- 1.6825605553965415
- 1.681618244417252
- 1.6863904729966195
- 1.6797623980429865
- 1.685507878180473
- 1.6902804182421776
- 1.6913575818461757
- 1.6796462112857449
- 1.671548435764928
- 1.6853625812838156
- 1.677775186877097
- 1.6852168229318434
- 1.6836326237647765
- 1.6783150049947924
- 1.6785905784176243
- 1.67934242756136
- 1.6860165942099787
- 1.6811724401289416
- 1.6843405077534337
- 1.6797924541657971
- 1.6815212888102378
- 1.6722007989883423
- 1.6826841715843446
- 1.6821610466126473
- 1.683530569076538
- 1.6839365036256853
- 1.681370565968175
- 1.6807761423049434
virtual_batch_multiplier: 0
vocab_size: 10000
