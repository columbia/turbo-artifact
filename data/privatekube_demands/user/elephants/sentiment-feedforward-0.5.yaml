accuracy: 0.7082458227133948
adaptive_batch_size: 1
alphas:
- 1.5
- 1.75
- 2
- 2.5
- 3
- 4
- 5
- 6
- 8
- 16
- 32
- 64
batch_size: 323
best_alpha: 64.0
delta: 1.0e-09
device: cuda
dp: 1
dp_eval: 0
dropout: 0.25
dynamic_clipping: 0
embedding_dim: 100
epsilon: 0.4965734815279016
hidden_dim: 100
hidden_dim_1: 150
hidden_dim_2: 110
learning_rate: 0.01
learning_rate_scheduler: 1
loss: 1.021077147200088
max_grad_norm: 1.0
max_text_len: 30
model: feedforward
n_blocks: 5000
n_blocks_test: 200
n_epochs: 60
n_train_users: 104866
n_trainable_parameters: 31871
n_workers: 6
noise: 5.991385498046875
non_dp_batch_size: 256
per_layer_clipping: 0
rdp_epsilons:
- 0.0039073547520729335
- 0.004558680411719251
- 0.005210034305608727
- 0.006512827990617976
- 0.007815735764311778
- 0.010421893642805652
- 0.013028508101919559
- 0.015635579302664106
- 0.020851092574047592
- 0.041731447607422804
- 0.08358027674667858
- 0.16763275395732366
target_epsilon: 0.5
task: sentiment
test_size: 39077
timeframe_days: 0
total_time: 160.8229329586029
train_size: 1332804
training_accuracy_epochs:
- 0.6514256605763494
- 0.6594713723953859
- 0.6971390701738405
- 0.7095898584819135
- 0.7089496424168716
- 0.7061976628539003
- 0.7122271706660589
- 0.7162404720797951
- 0.7164602487543483
- 0.7189351167575813
- 0.7172915747872105
- 0.7189924498031168
- 0.7182089007563062
- 0.7183808982372284
- 0.7178457918358437
- 0.7181897893731977
- 0.7182280125073445
- 0.7186293421704092
- 0.7184000105401616
- 0.7178649030349873
- 0.7181802325778537
- 0.7173584627884405
- 0.7181420123871461
- 0.7185720078371189
- 0.7187248961425122
- 0.7184668961498473
- 0.7188682304120358
- 0.7177502374958109
- 0.7179031248813794
- 0.7198237752840843
- 0.7178935688218953
- 0.718600673440062
- 0.7180177913403806
- 0.7178744572548219
- 0.7182471211309787
- 0.7178649028510223
- 0.7177693494308142
- 0.7188968943592943
- 0.7192122242700907
- 0.7178457931235984
- 0.7183904533768878
- 0.7185911210598769
- 0.7175400179845316
- 0.7173202409420485
- 0.7187248965104421
- 0.7194702251825804
- 0.7177597911637507
- 0.7188108962627104
- 0.716947576146067
- 0.7183140102359984
- 0.718705783839579
- 0.7192217803295747
- 0.7179317912201822
- 0.7189446726331005
- 0.7184477886300028
- 0.7179126779974243
- 0.7180655685103969
- 0.7183331223549666
- 0.7184573424819075
- 0.7184668961498473
training_loss_epochs:
- 0.9120927188131545
- 0.989942326221937
- 0.9526373631792304
- 0.9714837355746163
- 0.9909776311229777
- 0.9856377880514404
- 0.9778045141770516
- 0.9824174924029244
- 0.9755684632210084
- 0.9749256056032063
- 0.9816994343274905
- 0.9750575964097623
- 0.97616218895088
- 0.9754204761098932
- 0.9805040276712842
- 0.9785541378789477
- 0.9807657352936121
- 0.9792603653522185
- 0.9788481815729613
- 0.9801885673293361
- 0.9803870730561974
- 0.9802980847932674
- 0.9796192552204486
- 0.9771400233845652
- 0.9778928618740153
- 0.9775427364640765
- 0.9783861405319638
- 0.9825662295391531
- 0.9791843188397679
- 0.9747410767976149
- 0.982029308323507
- 0.9788010232610467
- 0.9796535188769117
- 0.9786781946450104
- 0.9801631739110123
- 0.9800583102084972
- 0.9803089685278175
- 0.9774419878735955
- 0.975830790437298
- 0.9790610821894657
- 0.9778678288430344
- 0.9769913044608669
- 0.9797816074188844
- 0.9850431355061354
- 0.9745615794334883
- 0.9740645311496876
- 0.9819756181519709
- 0.9778973576095369
- 0.9838960974672695
- 0.9769760684834586
- 0.9779558899226012
- 0.9766998995601395
- 0.9808494884658743
- 0.9758465676395981
- 0.979735764272419
- 0.9795842752044583
- 0.979926378822621
- 0.9791020736282254
- 0.9808326613755873
- 0.9762614866097769
training_time: 119.93502831459045
user_level: 1
validation_accuracy_epochs:
- 0.6531909180850517
- 0.6745029978635835
- 0.7080337027224098
- 0.70925257264114
- 0.704632117980864
- 0.7054841169496862
- 0.7051428861734343
- 0.7100689527465076
- 0.7122552278565197
- 0.7103674397235964
- 0.7119745510380443
- 0.7108960151672363
- 0.7110506019941191
- 0.7103567486856042
- 0.7113562124531444
- 0.7121291378649269
- 0.7125138160659046
- 0.7114317242692156
- 0.7115863038272392
- 0.7132831768291753
- 0.710504214938094
- 0.712204643865911
- 0.7105042120305504
- 0.7115863125498701
- 0.7112771374423329
- 0.7111225535229939
- 0.7123592350541091
- 0.7106587944961176
- 0.7120500672154311
- 0.7125138189734482
- 0.7125138160659046
- 0.7117408935616656
- 0.7122046496809983
- 0.710967968149883
- 0.7122046496809983
- 0.7117408979229811
- 0.7122046525885419
- 0.7131321575583481
- 0.7114317228154439
- 0.7132867443852309
- 0.7114317199079002
- 0.712359236507881
- 0.7117408979229811
- 0.7120500643078874
- 0.7118954774810047
- 0.7108133856843157
- 0.7103496266574394
- 0.7103496310187549
- 0.7098858763531941
- 0.7115863096423265
- 0.7108133798692284
- 0.7123592321465655
- 0.7105042120305504
- 0.7115863096423265
- 0.712359236507881
- 0.711431721361672
- 0.711431721361672
- 0.7118954789347764
- 0.7118954774810047
- 0.7114317242692156
validation_loss_epochs:
- 1.0560891642803099
- 1.0254005484464692
- 0.9173531939343709
- 0.9030166282886412
- 0.9071861098452312
- 0.9969331345907072
- 1.0190318750172127
- 0.9906691181950453
- 0.9763930454486753
- 1.008240315972305
- 0.9955470838197847
- 1.0100259053997878
- 1.0044211527196372
- 1.0077954196348422
- 1.006590238431605
- 1.0061881905648766
- 1.0015196872920524
- 1.0072847546600714
- 1.0086668904234724
- 1.002884847361867
- 1.0094361290699099
- 1.0046748635245533
- 1.0107668667304806
- 1.0038849856795333
- 1.0106152136151383
- 1.006640060645778
- 1.0012239464899388
- 1.0127584323650454
- 1.0044974423036344
- 1.0029292978891513
- 1.0039993917069785
- 1.0052971331084646
- 1.0046227617961605
- 1.0080721363788698
- 1.0017634702891838
- 1.0070585346803433
- 1.0038980550882293
- 1.0012921327497901
- 1.0046157371707078
- 1.0015919673733595
- 1.005769207710173
- 1.0037226880468972
- 1.0062856165374197
- 1.003612271169337
- 1.0034117335226478
- 1.011808552393099
- 1.0082580548960989
- 1.0120446463910544
- 1.0145448126444003
- 1.007701113456633
- 1.008446068298526
- 1.003014218516466
- 1.0089981788542213
- 1.0039601878422062
- 0.9997000316294228
- 1.008868883295757
- 1.0054137939360084
- 1.0047953070663824
- 1.0044555198855516
- 1.0050364354761636
virtual_batch_multiplier: 0
vocab_size: 10000
