accuracy: 0.606609912307895
adaptive_batch_size: 1
alphas:
- 1.5
- 1.75
- 2
- 2.5
- 3
- 4
- 5
- 6
- 8
- 16
- 32
- 64
batch_size: 160
best_alpha: 8.0
delta: 1.0e-09
device: cuda
dp: 1
dp_eval: 0
dropout: 0.25
dynamic_clipping: 0
embedding_dim: 100
epsilon: 4.9741828175039
hidden_dim: 100
hidden_dim_1: 240
hidden_dim_2: 195
learning_rate: 0.01
learning_rate_scheduler: 1
loss: 1.2094334597490273
max_grad_norm: 1.0
max_text_len: 75
model: bow
n_blocks: 500
n_blocks_test: 200
n_epochs: 60
n_train_users: 25671
n_trainable_parameters: 1111
n_workers: 6
noise: 1.108621826171875
non_dp_batch_size: 256
per_layer_clipping: 0
rdp_epsilons:
- 0.34943532717572356
- 0.40876894910193684
- 0.4684274397284847
- 0.5887387098351449
- 0.710410140098067
- 0.9580119678948786
- 1.211624823173879
- 1.4717102432329252
- 2.013716269368699
- 10497.769936929084
- 74654.57022462894
- 200428.6641576459
target_epsilon: 5.0
task: product
test_size: 39077
timeframe_days: 0
total_time: 81.20508909225464
train_size: 165215
training_accuracy_epochs:
- 0.43343750678468496
- 0.5367187600582838
- 0.5703906357288361
- 0.5794140739366412
- 0.590234388411045
- 0.5957812637090683
- 0.5980468882247806
- 0.6040625110268593
- 0.6092968869954347
- 0.6114453282207251
- 0.6146093841642142
- 0.6163671992719173
- 0.6196484487503767
- 0.6191406369209289
- 0.6214062612503767
- 0.6240625109523534
- 0.6241796988993883
- 0.6272265758365393
- 0.625859385728836
- 0.6262890737503767
- 0.6275390718132258
- 0.6287890747189522
- 0.6294921994209289
- 0.6287109479308128
- 0.6319531366229058
- 0.6319531362503767
- 0.633007824420929
- 0.6334375124424696
- 0.6341015722602605
- 0.6346875134855509
- 0.6340234469622373
- 0.6345703259110451
- 0.636992198228836
- 0.6378125134855509
- 0.6352343890815974
- 0.6372656363993883
- 0.6352343857288361
- 0.6364062607288361
- 0.6379687633365393
- 0.6382812626659871
- 0.6382031358778477
- 0.6391796998679637
- 0.6394140746444463
- 0.6401562593877316
- 0.6421875115483999
- 0.6394921984523535
- 0.6395312637090683
- 0.6428125124424696
- 0.6414062641561031
- 0.6451171997934579
- 0.6423047002404928
- 0.6412890754640103
- 0.6439062640070915
- 0.6442968856543303
- 0.641992200538516
- 0.6436718855053186
- 0.6429687656462193
- 0.6453515760600567
- 0.6445703249424696
- 0.6443359486758709
training_loss_epochs:
- 1.8090711556375028
- 1.5625639773905278
- 1.471336441487074
- 1.4230733938515185
- 1.3798841938376427
- 1.3554226748645306
- 1.337589405477047
- 1.31301099807024
- 1.2954698972404004
- 1.2825515024363994
- 1.2694916635751725
- 1.2618906535208225
- 1.2539563417434691
- 1.2471599079668523
- 1.2370164163410664
- 1.2274323619902134
- 1.2254179105162621
- 1.2186405643820764
- 1.2174422096461057
- 1.2129675272852183
- 1.2084053054451942
- 1.2025437340140344
- 1.1933937452733516
- 1.1922143507748841
- 1.1896872460842132
- 1.1841186746954917
- 1.17688378803432
- 1.173426579684019
- 1.1730696398764848
- 1.1691935330629348
- 1.1703186590224504
- 1.1654900137335062
- 1.1557556863874197
- 1.1562122456729411
- 1.1556231204420329
- 1.1513559170067311
- 1.1552479799836874
- 1.1540778938680887
- 1.147633418813348
- 1.1451251089572907
- 1.1414680369198322
- 1.1391963113099337
- 1.1387908678501844
- 1.137432425841689
- 1.1300647657364606
- 1.1341435600072145
- 1.1297115441411734
- 1.1259713318198918
- 1.125535112991929
- 1.1199976675212384
- 1.1244529962539673
- 1.1210361577570438
- 1.1175936065614223
- 1.1183301527053118
- 1.1192197378724813
- 1.1140541959553958
- 1.1149986408650876
- 1.112102236226201
- 1.1093432858586312
- 1.109303964674473
training_time: 55.47719383239746
user_level: 1
validation_accuracy_epochs:
- 0.4790511890882399
- 0.5403062733935147
- 0.5483208771159009
- 0.5554323832436305
- 0.5604628746102496
- 0.5648720584264616
- 0.5728104350770392
- 0.5738451706200112
- 0.5786770247831577
- 0.5856060705533842
- 0.5863659585394511
- 0.5818459093570709
- 0.5813562572002411
- 0.5879873546158395
- 0.5895348405692635
- 0.589927490528037
- 0.5913502523084966
- 0.5917336580956855
- 0.5888811989528376
- 0.5961105067555498
- 0.593826232886896
- 0.5931610450512026
- 0.6009215770698175
- 0.5946738894392805
- 0.5962421566974826
- 0.6021549450915035
- 0.5986927304326034
- 0.5967110260957624
- 0.6030026005535591
- 0.5986049633200575
- 0.6023605080639444
- 0.6029055914500865
- 0.6038848975809609
- 0.6008361149124983
- 0.5966232557122301
- 0.5964800562800431
- 0.6031342541299215
- 0.5989629644446257
- 0.6032081617087852
- 0.6026561441944867
- 0.6011109679210477
- 0.6047233110520898
- 0.603591567859417
- 0.604113551901608
- 0.6043953354038843
- 0.6037994390580712
- 0.6041920846555291
- 0.6068574575389304
- 0.6061275986636557
- 0.6072039109904591
- 0.606739664223136
- 0.6092618369474644
- 0.6085781702181188
- 0.6096129104858492
- 0.6101002555067946
- 0.6099709105200883
- 0.6095690240220326
- 0.6104166783937593
- 0.6094604686992925
- 0.6095251397388738
validation_loss_epochs:
- 1.7125522523391536
- 1.6004830715132923
- 1.5233885529564648
- 1.495220925749802
- 1.4670452911679337
- 1.4315600235287735
- 1.410734615674833
- 1.3945036326966636
- 1.3793482911296007
- 1.373951007680195
- 1.3469138930483562
- 1.3497535266527316
- 1.3484727929278117
- 1.3416441577236826
- 1.321817042624078
- 1.3205032261406504
- 1.310843389208724
- 1.3096938612984448
- 1.316452565716534
- 1.3022143441002543
- 1.2947354796456128
- 1.2924879782083558
- 1.280550397023922
- 1.280628897794863
- 1.2792952525906447
- 1.2684259574587753
- 1.2752998703863563
- 1.2712279034823906
- 1.2573780926262461
- 1.2669326503102372
- 1.2540970510098992
- 1.2405246307210225
- 1.2545831363375595
- 1.2525538060723282
- 1.252701065889219
- 1.2489201848099871
- 1.2449706422119606
- 1.2400302683434836
- 1.2424429407933864
- 1.2279318382100362
- 1.2285442381370357
- 1.2319617889276364
- 1.2428094381239356
- 1.2203257549099806
- 1.2302796855205442
- 1.2250090692101456
- 1.2234023009858481
- 1.2135374785923376
- 1.2145719935254353
- 1.218925854781779
- 1.2096125527126034
- 1.2099214145323125
- 1.212205378747568
- 1.200484054844554
- 1.2007111100161947
- 1.2027899477539994
- 1.196933768871354
- 1.1927269646307317
- 1.2020838522329562
- 1.1978359164261236
virtual_batch_multiplier: 0
vocab_size: 10000
