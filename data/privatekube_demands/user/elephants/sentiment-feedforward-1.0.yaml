accuracy: 0.7232366455487969
adaptive_batch_size: 1
alphas:
- 1.5
- 1.75
- 2
- 2.5
- 3
- 4
- 5
- 6
- 8
- 16
- 32
- 64
batch_size: 323
best_alpha: 64.0
delta: 1.0e-09
device: cuda
dp: 1
dp_eval: 0
dropout: 0.25
dynamic_clipping: 0
embedding_dim: 100
epsilon: 0.9941723967623615
hidden_dim: 100
hidden_dim_1: 150
hidden_dim_2: 110
learning_rate: 0.01
learning_rate_scheduler: 1
loss: 1.0310780213884085
max_grad_norm: 1.0
max_text_len: 30
model: feedforward
n_blocks: 5000
n_blocks_test: 200
n_epochs: 60
n_train_users: 104866
n_trainable_parameters: 31871
n_workers: 6
noise: 3.0922445678710937
non_dp_batch_size: 256
per_layer_clipping: 0
rdp_epsilons:
- 0.015246720709001238
- 0.017789400585570547
- 0.0203325266596955
- 0.025420117700272048
- 0.030509495073951044
- 0.0406936139769089
- 0.05088489372168824
- 0.06108334468557398
- 0.0815018019011586
- 0.16346457177153687
- 0.32879474723512436
- 0.6652316691917836
target_epsilon: 1.0
task: sentiment
test_size: 39077
timeframe_days: 0
total_time: 156.9881627559662
train_size: 1332804
training_accuracy_epochs:
- 0.6519798774410177
- 0.6909566749761133
- 0.7134884941725084
- 0.7210277657449982
- 0.7218399814985417
- 0.7254519539850729
- 0.7282421546585766
- 0.7280032662329851
- 0.7272388282013528
- 0.730468581488103
- 0.7295608064274729
- 0.7300194726314073
- 0.7297996959568541
- 0.7295416989076284
- 0.7299143614960305
- 0.73031569244685
- 0.7309272432768786
- 0.7303061374911556
- 0.7297519184189079
- 0.7302296921426867
- 0.731070574602963
- 0.7301341365148992
- 0.7298474729429056
- 0.7299239175555147
- 0.729579918914371
- 0.73098457356294
- 0.7312234619885315
- 0.7311565726995468
- 0.7300003603284742
- 0.7308316867292663
- 0.7312330178640507
- 0.7312330174961208
- 0.7303443574978982
- 0.7300194718955476
- 0.7313572378070267
- 0.7299430261791489
- 0.730468577992769
- 0.729971695093461
- 0.7304685785446638
- 0.7287008160794223
- 0.7307265768816442
- 0.7295225862367654
- 0.730678798607838
- 0.7297136949168311
- 0.7301914710321544
- 0.7318827903565065
- 0.7293888080267259
- 0.7303825800801501
- 0.7308699096794482
- 0.7303539126375576
- 0.7295608086350523
- 0.7300672505372836
- 0.730000361064334
- 0.7313859045137594
- 0.7296372517759417
- 0.7311470186636772
- 0.731032352020711
- 0.7310419071603704
- 0.7310132408215676
- 0.7299143594724161
training_loss_epochs:
- 0.891069468156791
- 0.9139328519871206
- 0.9505772049780246
- 1.0124037170115812
- 1.0268423816672079
- 1.0346184836493597
- 1.0236002208642017
- 1.0283443993992276
- 1.0224660714099436
- 1.0019689833308443
- 1.0055767121138397
- 1.0032219058937497
- 1.0053006362767867
- 1.007892996440699
- 1.0062369634339838
- 1.0035228158956693
- 1.0024368126451233
- 1.0046484391262502
- 1.0060607062445746
- 1.0020940309321438
- 1.0035080584111038
- 1.00505113804046
- 1.0076992923830763
- 1.0051287922226353
- 1.0060688682176449
- 1.0040555239459614
- 1.0025132923950384
- 1.0033132214973002
- 1.0045526583253601
- 1.0051830488222617
- 1.0032417807314131
- 1.0033820486731
- 1.0003944558125955
- 1.0028074909875422
- 0.9995730495379295
- 1.0030809732866877
- 1.0022305347669271
- 1.0053943358821633
- 1.0044913819910568
- 1.0077465428614323
- 1.001286464156928
- 1.0056842495629816
- 1.0045014529684444
- 1.0037345367449302
- 1.0043892016013463
- 0.9992576541356099
- 1.0094735754860773
- 1.0033219940500495
- 1.0039638188886053
- 1.0040878315030792
- 1.0049721948158594
- 1.0084657006793551
- 1.0032551593986558
- 0.9989021253070713
- 1.0100066365651141
- 1.0009752307777051
- 1.0013834219655873
- 1.0050202289849153
- 1.0016180328380915
- 1.0100252977859827
training_time: 117.25896430015564
user_level: 1
validation_accuracy_epochs:
- 0.6547802250559737
- 0.7053117228717338
- 0.7116618214583978
- 0.7133807772543372
- 0.7214241871019689
- 0.7204753015099502
- 0.7212517857551575
- 0.7222334393640844
- 0.7227185717443141
- 0.7227114409935184
- 0.722470661488975
- 0.7224058363495803
- 0.7224813481656517
- 0.7213273033863161
- 0.7218665655066328
- 0.7211727209207488
- 0.7205579339004144
- 0.7211762797541734
- 0.7207089604401007
- 0.7196268642820963
- 0.7213272990250006
- 0.7224093966367768
- 0.7225639834636595
- 0.7210181355476379
- 0.721172719466977
- 0.7208635472669834
- 0.7210181326400943
- 0.7210181311863225
- 0.7217910522367896
- 0.7219456434249878
- 0.7210181326400943
- 0.7219456390636724
- 0.721172719466977
- 0.7227185673829986
- 0.7216364741325378
- 0.7219456434249878
- 0.7217910580518769
- 0.721481888759427
- 0.7196268642820963
- 0.7213273062938597
- 0.7222548156249814
- 0.7221002273443269
- 0.7211727136518897
- 0.7213273004787725
- 0.7213272990250006
- 0.721945641971216
- 0.7222548127174377
- 0.7217910551443333
- 0.7217910595056487
- 0.7205543721594462
- 0.7214818873056551
- 0.7224093980905486
- 0.7228731542098813
- 0.7207089677089598
- 0.7213273062938597
- 0.7227185644754549
- 0.7216364712249942
- 0.7208635487207552
- 0.7216364741325378
- 0.7220211552410591
validation_loss_epochs:
- 1.0810566399155594
- 1.037288776258143
- 0.9596813815396007
- 0.9893839809952713
- 1.0326825729230555
- 1.0746967588982932
- 1.0823827967411135
- 1.088161545555766
- 1.0436568856239319
- 1.04187340125805
- 1.046615266218418
- 1.0412777022617619
- 1.0452738360660832
- 1.0418690341274912
- 1.043550655609224
- 1.044790365347048
- 1.0433119738974221
- 1.0419613675373356
- 1.0395319650812846
- 1.0439785413625764
- 1.0418461808344213
- 1.0410906846930341
- 1.0385340961014353
- 1.0447195433988803
- 1.0431957172184456
- 1.041616966084736
- 1.044142435236675
- 1.0418467565280636
- 1.0416431252549334
- 1.044063719307504
- 1.045435310863867
- 1.0404588129462264
- 1.0398868976569757
- 1.0397265568012144
- 1.038935744180912
- 1.0395563070367022
- 1.0419326482749567
- 1.0403658573220416
- 1.049357755881984
- 1.0369405891837142
- 1.0399341466950207
- 1.0397053201024125
- 1.0410016280848806
- 1.0398054748046688
- 1.0432117435990311
- 1.0398585113083445
- 1.0410964881501548
- 1.0394243045551022
- 1.0425969333183476
- 1.0427149519687746
- 1.0377272207562516
- 1.037768989074521
- 1.0384114864395886
- 1.0445370078086853
- 1.0417471164610328
- 1.0370363971082175
- 1.039781852466304
- 1.0428047180175781
- 1.0439871622294914
- 1.0390211227463513
virtual_batch_multiplier: 0
vocab_size: 10000
