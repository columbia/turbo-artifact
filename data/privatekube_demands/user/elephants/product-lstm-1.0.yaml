accuracy: 0.6310124747024095
adaptive_batch_size: 1
alphas:
- 1.5
- 1.75
- 2
- 2.5
- 3
- 4
- 5
- 6
- 8
- 16
- 32
- 64
batch_size: 323
best_alpha: 64.0
delta: 1.0e-09
device: cuda
dp: 1
dp_eval: 0
dropout: 0.25
dynamic_clipping: 0
embedding_dim: 100
epsilon: 0.9941723967623615
hidden_dim: 40
hidden_dim_1: 240
hidden_dim_2: 195
learning_rate: 0.01
learning_rate_scheduler: 1
loss: 1.3188627909037693
max_grad_norm: 1.0
max_text_len: 30
model: lstm
n_blocks: 5000
n_blocks_test: 200
n_epochs: 60
n_train_users: 104866
n_trainable_parameters: 23171
n_workers: 6
noise: 3.0922445678710937
non_dp_batch_size: 256
per_layer_clipping: 0
rdp_epsilons:
- 0.015246720709001238
- 0.017789400585570547
- 0.0203325266596955
- 0.025420117700272048
- 0.030509495073951044
- 0.0406936139769089
- 0.05088489372168824
- 0.06108334468557398
- 0.0815018019011586
- 0.16346457177153687
- 0.32879474723512436
- 0.6652316691917836
target_epsilon: 1.0
task: product
test_size: 39077
timeframe_days: 0
total_time: 838.9652142524719
train_size: 1332804
training_accuracy_epochs:
- 0.5100714587089088
- 0.561508603486014
- 0.5644421347497423
- 0.5671558889709873
- 0.5767687009072598
- 0.5829415420691172
- 0.5848144154857706
- 0.5906432565347648
- 0.5952967758531924
- 0.5974658700051131
- 0.5959943279072091
- 0.597943643544927
- 0.6002465138832728
- 0.599950294619725
- 0.6018996126489875
- 0.6018136125287892
- 0.6013931725864057
- 0.6053395861828769
- 0.6091235559663655
- 0.6085215586203115
- 0.6098115477297041
- 0.610098213693242
- 0.6165290520882901
- 0.6173699330768467
- 0.6214501241474976
- 0.6217463417553607
- 0.6207047949234644
- 0.621373677695239
- 0.6215839001499577
- 0.6206761284006966
- 0.6231605528313436
- 0.6276516290726485
- 0.6301838314091718
- 0.6326109213225636
- 0.631836931646606
- 0.6329931410742394
- 0.633566470057876
- 0.6344646857476529
- 0.6345029085138698
- 0.6365955580531815
- 0.6362802283263501
- 0.637942878552425
- 0.6399973077170643
- 0.6410579664839638
- 0.6415548501191316
- 0.6415930734372434
- 0.639509978853626
- 0.6379906560903714
- 0.6410866320869069
- 0.6436475016820578
- 0.6458165956500136
- 0.6464472562074661
- 0.6469632526974619
- 0.6471543615614926
- 0.6481003533174963
- 0.6489221247625939
- 0.6494094527062074
- 0.649543231100212
- 0.6511103297457282
- 0.6515594374986342
training_loss_epochs:
- 1.7847091894090912
- 1.6142029519434329
- 1.6414829834743783
- 1.5954537553551755
- 1.5287374298513672
- 1.5203522193579027
- 1.504901103399418
- 1.4949567730044142
- 1.4821586009160972
- 1.4736045190581568
- 1.4584566910325745
- 1.4454645727887565
- 1.4511371626530165
- 1.4451728025336323
- 1.433756685551302
- 1.4303395652476651
- 1.4308614193657299
- 1.4403696652547813
- 1.4132147280522336
- 1.4147276904112027
- 1.409821394417021
- 1.390737838583228
- 1.376860968860579
- 1.379580040404826
- 1.3793245193399029
- 1.3797425789597593
- 1.3720891229164454
- 1.3647606284530074
- 1.3728120198220382
- 1.3510353035397
- 1.3696364477092837
- 1.3685994424201824
- 1.3419294927591159
- 1.3499415572042819
- 1.342218319207062
- 1.332115434202147
- 1.3259549221874756
- 1.3394334847912377
- 1.3204755893460027
- 1.334097448322508
- 1.3299772331008204
- 1.3092196392424313
- 1.3062583623844901
- 1.3120336661368242
- 1.297675046287937
- 1.3070088654388616
- 1.3155236286513599
- 1.3185854373890677
- 1.316366723290196
- 1.315392901500066
- 1.2967623527403231
- 1.2907969259921415
- 1.2859982673400714
- 1.2779173843654585
- 1.285614028757001
- 1.2786756777836952
- 1.2792489819320632
- 1.2694041817276567
- 1.2621984084447224
- 1.26718206188561
training_time: 790.9384114742279
user_level: 1
validation_accuracy_epochs:
- 0.5233152552348811
- 0.5332286452374807
- 0.5407007314809938
- 0.5281658092649971
- 0.5574030410952684
- 0.5585392742622189
- 0.5612505717975337
- 0.5680893514214492
- 0.571369123168108
- 0.5688230976825808
- 0.5673741320284401
- 0.5688195366684984
- 0.5654037027824216
- 0.5688195373953843
- 0.5790278533609902
- 0.5758599231882793
- 0.5760251952380668
- 0.5809769063461118
- 0.5755471936086329
- 0.5814584682627422
- 0.5753890421332383
- 0.5852775079448048
- 0.5878413363200862
- 0.5902719526756101
- 0.5976471595647859
- 0.5982797436597871
- 0.5970679914079061
- 0.5953853726387024
- 0.5925444233708266
- 0.5907072163209682
- 0.5998476859999866
- 0.607876841614886
- 0.6117314899840006
- 0.6126661257046025
- 0.6099085313517872
- 0.6093941970569331
- 0.6029365295317115
- 0.6064207481174935
- 0.6132274764340099
- 0.6114294470810309
- 0.6072976821806373
- 0.6150433188531457
- 0.6142953227205974
- 0.6152121555514452
- 0.6209367848024135
- 0.6164815978306096
- 0.6181713531656963
- 0.6162985185297524
- 0.6254995363514598
- 0.6224078347043294
- 0.6256042573510147
- 0.6245292846749468
- 0.6285848297723909
- 0.6306813402873713
- 0.6288512552656779
- 0.6320832880531869
- 0.6305089505707345
- 0.6320547955792125
- 0.6329645005668082
- 0.6340936131593657
validation_loss_epochs:
- 1.881349578136351
- 1.7198756904136845
- 1.6520078036843278
- 1.6834656523495186
- 1.6176105446931792
- 1.5924447949339704
- 1.59154400592897
- 1.525915009219472
- 1.5524513808692373
- 1.5473319931728085
- 1.5662333383792784
- 1.5473456121072537
- 1.5238149951143962
- 1.4956182735722239
- 1.4658999908261183
- 1.4820161970650279
- 1.5451165379547491
- 1.4763153529748685
- 1.5165235589190227
- 1.4813340262668888
- 1.5076137897444934
- 1.4560454502338316
- 1.4886088778332967
- 1.453918169184429
- 1.4432711252352086
- 1.4205307960510254
- 1.4095865051920822
- 1.4163463987955234
- 1.400423032481496
- 1.4340730556627599
- 1.4537472230632131
- 1.375928058856871
- 1.3781425080648282
- 1.3601122193220185
- 1.3776482779805252
- 1.4036116222055948
- 1.4195887548167532
- 1.3861821250217716
- 1.3380488680630196
- 1.410171613460634
- 1.3765065262957317
- 1.3693708850116264
- 1.3560558063227957
- 1.3558471202850342
- 1.3573155897419626
- 1.383405574938146
- 1.3865411048982201
- 1.3439923321328513
- 1.4062392595337658
- 1.3695825861721505
- 1.3541145441008777
- 1.3591388144144199
- 1.3384419156283867
- 1.3494012850086863
- 1.3432053850918282
- 1.3360898291192405
- 1.3391089148637725
- 1.3199672757125482
- 1.3225224512379343
- 1.3291280182396494
virtual_batch_multiplier: 0
vocab_size: 10000
