accuracy: 0.7111305715162543
adaptive_batch_size: 1
alphas:
- 1.5
- 1.75
- 2
- 2.5
- 3
- 4
- 5
- 6
- 8
- 16
- 32
- 64
batch_size: 124
best_alpha: 8.0
delta: 1.0e-09
device: cuda
dp: 1
dp_eval: 0
dropout: 0.25
dynamic_clipping: 0
embedding_dim: 100
epsilon: 4.9670336217042745
hidden_dim: 100
hidden_dim_1: 150
hidden_dim_2: 110
learning_rate: 0.01
learning_rate_scheduler: 1
loss: 0.9119236582442175
max_grad_norm: 1.0
max_text_len: 30
model: feedforward
n_blocks: 200
n_blocks_test: 200
n_epochs: 60
n_train_users: 15554
n_trainable_parameters: 31871
n_workers: 6
noise: 1.2078029632568361
non_dp_batch_size: 256
per_layer_clipping: 0
rdp_epsilons:
- 0.3502699785876049
- 0.4096805233886432
- 0.46939612566888367
- 0.5897586919379566
- 0.7113911412729209
- 0.9586090171400296
- 1.211358224253727
- 1.4699880706182387
- 2.006567073569073
- 2514.17506167215
- 44852.65014539943
- 127706.48618075538
target_epsilon: 5.0
task: sentiment
test_size: 39077
timeframe_days: 0
total_time: 83.56373620033264
train_size: 64934
training_accuracy_epochs:
- 0.6412257871627808
- 0.6408386883735657
- 0.6519354629516602
- 0.6799999823570252
- 0.6960644917488098
- 0.699806429862976
- 0.7023870759010314
- 0.7068386898040772
- 0.7054193325042725
- 0.7038709454536438
- 0.7070322389602661
- 0.7099354639053345
- 0.7085161085128784
- 0.7090967507362366
- 0.7083225617408753
- 0.7074193334579468
- 0.7090322365760803
- 0.709935462474823
- 0.7090322351455689
- 0.7103870754241943
- 0.7094838495254516
- 0.7096773962974549
- 0.7075483675003051
- 0.7079354615211487
- 0.7080644950866699
- 0.7082580442428589
- 0.7098064351081849
- 0.7067096567153931
- 0.7079999785423279
- 0.7074838514328003
- 0.7072257852554321
- 0.7104515924453735
- 0.7085161066055298
- 0.7089677238464356
- 0.7083225584030152
- 0.7064515900611877
- 0.7066451411247253
- 0.7090967521667481
- 0.7087096552848816
- 0.7074193315505981
- 0.709225784778595
- 0.7079999794960022
- 0.7114193329811096
- 0.7109032053947448
- 0.7074838480949401
- 0.711870945930481
- 0.7061935286521912
- 0.7052903017997741
- 0.7079999799728394
- 0.7085806250572204
- 0.7072903003692627
- 0.7090322351455689
- 0.709483847618103
- 0.7081935267448425
- 0.7099999785423279
- 0.7069677195549011
- 0.7099999785423279
- 0.7069677195549011
- 0.711677399635315
- 0.7078709483146668
training_loss_epochs:
- 0.805842348575592
- 0.9206808338165283
- 0.9346221632957459
- 0.9104345824718475
- 0.9236858491897583
- 0.9162656145095825
- 0.9097718119621276
- 0.905762231349945
- 0.9054738850593567
- 0.9319526534080506
- 0.9066122922897338
- 0.8958170766830444
- 0.9065441832542419
- 0.9067582945823669
- 0.9104911398887634
- 0.9052736282348632
- 0.9109565644264221
- 0.8992217216491699
- 0.9043258085250855
- 0.8978708972930908
- 0.9000027141571045
- 0.9008578763008118
- 0.9064228739738465
- 0.9038502898216247
- 0.9064799752235413
- 0.9074396567344666
- 0.89842498254776
- 0.9079354915618897
- 0.9076479234695435
- 0.906895525932312
- 0.9117880001068115
- 0.9024504551887512
- 0.9020278191566468
- 0.9041579439640045
- 0.9009497237205505
- 0.9108114256858826
- 0.9050767064094544
- 0.9037899351119996
- 0.9009074187278747
- 0.9090944294929504
- 0.9007575612068176
- 0.9016005067825318
- 0.8934017138481141
- 0.8945141201019287
- 0.9099354920387268
- 0.893298620223999
- 0.9102510199546814
- 0.9203007225990295
- 0.9104154391288757
- 0.9037871785163879
- 0.9094620213508606
- 0.905101342201233
- 0.9048920340538025
- 0.9050808501243591
- 0.8989409275054931
- 0.9111268444061279
- 0.8994253888130188
- 0.9138221883773804
- 0.8974575986862182
- 0.9089786629676819
training_time: 59.71961545944214
user_level: 1
validation_accuracy_epochs:
- 0.652008500301613
- 0.6550770767454831
- 0.6718654671929917
- 0.7002687988416204
- 0.7010803194540851
- 0.710463563788612
- 0.7082825944108783
- 0.7112497237493407
- 0.7093984355341714
- 0.7127459729617497
- 0.7114018854105247
- 0.7138618168965826
- 0.7124416485147657
- 0.7106410838523001
- 0.7136335738424985
- 0.7121373257547055
- 0.7125177293453576
- 0.7140139768708427
- 0.7110214829444885
- 0.7094491551507194
- 0.7140139757462267
- 0.7125177315945895
- 0.7140139734969949
- 0.7110214846314125
- 0.7080289915485202
- 0.7110214835067965
- 0.7080289932354441
- 0.7095252382305434
- 0.7140139796823826
- 0.7080289898615963
- 0.7095252387928512
- 0.7110214851937204
- 0.7095252371059274
- 0.7095252354190035
- 0.7140139768708427
- 0.7125177338438214
- 0.7125177327192055
- 0.7110214868806443
- 0.7125177315945895
- 0.7110214835067965
- 0.7125177338438214
- 0.7125177315945895
- 0.7125177293453576
- 0.7110214846314125
- 0.7125177344061294
- 0.7110214868806443
- 0.7140139785577666
- 0.7125177287830496
- 0.7125177321568975
- 0.7095252399174672
- 0.7125177321568975
- 0.7110214846314125
- 0.7095252382305434
- 0.7125177321568975
- 0.7125177327192055
- 0.7110214835067965
- 0.7110214806952566
- 0.7140139779954586
- 0.7125177366553612
- 0.7140139808069985
validation_loss_epochs:
- 0.949165162612807
- 1.0781518232147649
- 0.9229420838209818
- 0.8686911081368068
- 0.9441632642498556
- 0.8756432921256659
- 0.9225396885062164
- 0.893365961763094
- 0.974722793923234
- 0.9054579709498387
- 0.9087215296502383
- 0.9084013072106073
- 0.918922411945631
- 0.9211370961846046
- 0.9105281654815629
- 0.9189664125442505
- 0.9156925329622233
- 0.9097774727445729
- 0.9146504497753
- 0.9249695764397675
- 0.9104872576470645
- 0.9171335714043312
- 0.9111823518321199
- 0.9228077418399308
- 0.9227645104786135
- 0.9220399029974667
- 0.9308634653406324
- 0.9253330174482094
- 0.9101421458400646
- 0.9201904642132094
- 0.930203637424505
- 0.9186000913943885
- 0.9223285227451684
- 0.92166061952429
- 0.9101890262286618
- 0.9106153660225418
- 0.9199430672627575
- 0.9172506073735794
- 0.9119997969213521
- 0.9244077306873394
- 0.911607323108979
- 0.9122046535869814
- 0.9132925125787843
- 0.9149372077213144
- 0.9115895392759791
- 0.9165852862708973
- 0.9097312472338946
- 0.9111781100619514
- 0.9105541791837171
- 0.9275907390522506
- 0.9177947803488318
- 0.9193893541704934
- 0.9185141769220244
- 0.9169010756150732
- 0.9184475608591763
- 0.9123658729049394
- 0.9134286428397557
- 0.9096748083126995
- 0.9154355407885786
- 0.9097527582549824
virtual_batch_multiplier: 0
vocab_size: 10000
