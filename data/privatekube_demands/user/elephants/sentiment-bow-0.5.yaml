accuracy: 0.7145435081040563
adaptive_batch_size: 1
alphas:
- 1.5
- 1.75
- 2
- 2.5
- 3
- 4
- 5
- 6
- 8
- 16
- 32
- 64
batch_size: 323
best_alpha: 64.0
delta: 1.0e-09
device: cuda
dp: 1
dp_eval: 0
dropout: 0.25
dynamic_clipping: 0
embedding_dim: 100
epsilon: 0.4965734815279016
hidden_dim: 100
hidden_dim_1: 240
hidden_dim_2: 195
learning_rate: 0.01
learning_rate_scheduler: 1
loss: 0.5730204779254503
max_grad_norm: 1.0
max_text_len: 50
model: bow
n_blocks: 5000
n_blocks_test: 200
n_epochs: 60
n_train_users: 104866
n_trainable_parameters: 101
n_workers: 6
noise: 5.991385498046875
non_dp_batch_size: 256
per_layer_clipping: 0
rdp_epsilons:
- 0.0039073547520729335
- 0.004558680411719251
- 0.005210034305608727
- 0.006512827990617976
- 0.007815735764311778
- 0.010421893642805652
- 0.013028508101919559
- 0.015635579302664106
- 0.020851092574047592
- 0.041731447607422804
- 0.08358027674667858
- 0.16763275395732366
target_epsilon: 0.5
task: sentiment
test_size: 39077
timeframe_days: 0
total_time: 129.56125140190125
train_size: 1332804
training_accuracy_epochs:
- 0.651406549009276
- 0.6512823275945805
- 0.6558307344898765
- 0.6653193244595587
- 0.6800443180549292
- 0.6901635711575732
- 0.6963650762666891
- 0.7021652524486001
- 0.7039999047547211
- 0.7071914354592194
- 0.7095038583617151
- 0.7111760681426084
- 0.7115391777989305
- 0.7137082699272368
- 0.7155429233371475
- 0.7153900342958944
- 0.7173966848187976
- 0.7174062403263869
- 0.7188873394035998
- 0.7204735488803299
- 0.7205499909174296
- 0.7227381979241784
- 0.721352654290788
- 0.7219259794111605
- 0.7206646571924658
- 0.7216966496205625
- 0.7197759997697524
- 0.7195084429817435
- 0.7200817754607142
- 0.7201104395919375
- 0.7198906651249638
- 0.7203111037795926
- 0.7198046661085553
- 0.7211519884474483
- 0.7189064498668836
- 0.7199288862354961
- 0.7200531078341567
- 0.7201008846362432
- 0.7212953201414626
- 0.72028243670493
- 0.7198142218001095
- 0.7196040017369353
- 0.7208557684480408
- 0.719661334046611
- 0.7209131007577166
- 0.7201486592307503
- 0.7205691035882926
- 0.7210468767601766
- 0.7205404361457001
- 0.7193364473404708
- 0.7191357829688508
- 0.7203684384808128
- 0.720559549184493
- 0.7204162147310045
- 0.7204544374972214
- 0.7199193318316965
- 0.7194320013125738
- 0.7198811083296199
- 0.7201008839003834
- 0.7192313362050939
training_loss_epochs:
- 0.6663994178359891
- 0.6344055216989399
- 0.616000908392447
- 0.6016665332847171
- 0.5882690467952211
- 0.5829629224759562
- 0.5777818987398972
- 0.5737379311043539
- 0.5733360712542946
- 0.568736207338027
- 0.5680279008768223
- 0.5672745801232479
- 0.5662990958418375
- 0.5665659992783157
- 0.5650529923078454
- 0.5647295271908795
- 0.5619989512143312
- 0.5620449690355195
- 0.5622658853729566
- 0.5603840020887646
- 0.5596392347856805
- 0.5574127914912906
- 0.5593961912356777
- 0.5601176677478684
- 0.5603596690444299
- 0.5603573970772602
- 0.5601673828046999
- 0.560342211506249
- 0.5597947784412054
- 0.5601485947399963
- 0.560054907736219
- 0.5587895496943851
- 0.5598620644506113
- 0.5587556809186935
- 0.5605048154607232
- 0.5589331946807143
- 0.5599934716283539
- 0.5592818722864727
- 0.558166365104693
- 0.5586041251634374
- 0.5596256417072849
- 0.5605530508874376
- 0.5588866018770654
- 0.560168104959123
- 0.5589955381037276
- 0.5589152193731732
- 0.5589740538486728
- 0.5582842916066264
- 0.5591577126840015
- 0.5602999186994116
- 0.5595923148003625
- 0.5595363568008682
- 0.5584715437373997
- 0.5588919574647774
- 0.5602846291882021
- 0.5594490161280573
- 0.5603956902100716
- 0.5596835435724553
- 0.5594170902982171
- 0.5600498255204271
training_time: 85.77793955802917
user_level: 1
validation_accuracy_epochs:
- 0.6536546727506126
- 0.6557476287934838
- 0.6579766404337999
- 0.6711911704482102
- 0.6836577101451594
- 0.6863583195500258
- 0.6872430807206689
- 0.6894293529231373
- 0.6989074872761238
- 0.6970018831694998
- 0.6981922547991682
- 0.7047254370480049
- 0.704664178010894
- 0.708181166067356
- 0.7066061046065354
- 0.7119132832783025
- 0.712060750984564
- 0.7104066159667038
- 0.7091051151112813
- 0.7145205765235715
- 0.7149700842252592
- 0.7130929769539252
- 0.714085315785757
- 0.7084725208398772
- 0.7074944362407778
- 0.70956174193359
- 0.710255593788333
- 0.7104643161703901
- 0.7100831982566089
- 0.7104607573369655
- 0.7108454384454866
- 0.7103026029540271
- 0.7113056313700792
- 0.7089113447724319
- 0.7095296891724191
- 0.709917929114365
- 0.7101444689238944
- 0.7092169552314572
- 0.7104536353088007
- 0.710453632401257
- 0.7092169537776853
- 0.7109173914281334
- 0.7110719768012442
- 0.7110719724399287
- 0.7113811446399223
- 0.7104536338550288
- 0.7116903139323723
- 0.7110719738937006
- 0.7096807128045617
- 0.7110719753474724
- 0.7104536353088007
- 0.7106082221356834
- 0.7099898820970116
- 0.7109173870668178
- 0.7098352938163571
- 0.7119994875861377
- 0.711535724197946
- 0.7098352909088135
- 0.711226554905496
- 0.7102990499356898
validation_loss_epochs:
- 0.6417597721262676
- 0.6207164249769072
- 0.63097179226759
- 0.6082830022021037
- 0.5936234375325645
- 0.5969046717736779
- 0.597315301255482
- 0.6014126829984712
- 0.5855274709259591
- 0.5865388498073671
- 0.5844248329720846
- 0.5808588397212144
- 0.580620338277119
- 0.5784893181265854
- 0.5834058726706156
- 0.5755891567323266
- 0.5773453843302843
- 0.5765102978159742
- 0.5818477200298775
- 0.5744506811223379
- 0.5745906350089283
- 0.5749876949845291
- 0.5763262343115922
- 0.5838037162292294
- 0.5867797521556296
- 0.5793836749181515
- 0.5797676255063313
- 0.5787657164945835
- 0.5777126637900748
- 0.5782759240487727
- 0.5774118144337724
- 0.5785619994489158
- 0.5776650200529796
- 0.5793230591750727
- 0.5790204115030242
- 0.5788952940847816
- 0.5795019341678154
- 0.5802678189626554
- 0.5781558504918727
- 0.5786648029234351
- 0.5795771101625954
- 0.5782066263803621
- 0.5783957650021809
- 0.5785674452781677
- 0.5771385053308998
- 0.5793568041266465
- 0.5767696209070159
- 0.5777304754024599
- 0.5789082588219061
- 0.5777024801184492
- 0.5782080757908705
- 0.5785340608620062
- 0.5783418024458536
- 0.578089094743496
- 0.5781232845492479
- 0.5768479406833649
- 0.5767082194002663
- 0.5798632764234776
- 0.5780867963302426
- 0.5785309955841158
virtual_batch_multiplier: 0
vocab_size: 10000
