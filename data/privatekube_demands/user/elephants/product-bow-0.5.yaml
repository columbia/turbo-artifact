accuracy: 0.635990579922994
adaptive_batch_size: 1
alphas:
- 1.5
- 1.75
- 2
- 2.5
- 3
- 4
- 5
- 6
- 8
- 16
- 32
- 64
batch_size: 940
best_alpha: 64.0
delta: 1.0e-09
device: cuda
dp: 1
dp_eval: 0
dropout: 0.25
dynamic_clipping: 0
embedding_dim: 100
epsilon: 0.4982643119183256
hidden_dim: 100
hidden_dim_1: 240
hidden_dim_2: 195
learning_rate: 0.01
learning_rate_scheduler: 1
loss: 1.088213100319817
max_grad_norm: 1.0
max_text_len: 75
model: bow
n_blocks: 50000
n_blocks_test: 200
n_epochs: 60
n_train_users: 884394
n_trainable_parameters: 1111
n_workers: 6
noise: 3.5500036621093747
non_dp_batch_size: 256
per_layer_clipping: 0
rdp_epsilons:
- 0.003946132685261561
- 0.0046039253595341134
- 0.005261747420316885
- 0.00657748051935414
- 0.00789333223449005
- 0.010525391352219632
- 0.013157924947970353
- 0.01579093319682122
- 0.0210583743541596
- 0.042147160168311625
- 0.08441632651122556
- 0.16932358434774764
target_epsilon: 0.5
task: product
test_size: 39077
timeframe_days: 0
total_time: 503.98359084129333
train_size: 12869902
training_accuracy_epochs:
- 0.5650961782505854
- 0.6200260091969307
- 0.6296106637158292
- 0.635698260335212
- 0.641072863086741
- 0.645218404683661
- 0.6492371904089096
- 0.6526233371902019
- 0.6552466968272594
- 0.6575180853934999
- 0.6602184032506131
- 0.661586669404456
- 0.6623517210813279
- 0.6633634917279507
- 0.6642258718292764
- 0.6643956322619256
- 0.6657356053590775
- 0.6662663883985357
- 0.6662516767040212
- 0.6671898826005611
- 0.6681246952173557
- 0.6677704634184533
- 0.6683419882617099
- 0.6684313948484177
- 0.6688920107927728
- 0.6685015629580681
- 0.6688082626525392
- 0.6691285430116857
- 0.6692451116252452
- 0.6691839978415915
- 0.6693152786569393
- 0.6693922370672226
- 0.6689689683787367
- 0.6689633105663543
- 0.6687935498166592
- 0.6690085785186037
- 0.6687697827816009
- 0.6688580593530168
- 0.6687958131445215
- 0.6692541648732855
- 0.6691998420243568
- 0.6691194891295534
- 0.6689882078069321
- 0.6690210284070766
- 0.6689101183034004
- 0.6688863517756157
- 0.668875034185166
- 0.6687731798024887
- 0.6691330710624127
- 0.6691115672283984
- 0.6689112511086971
- 0.6691115672283984
- 0.6689734952880981
- 0.6691466506491316
- 0.669038003936727
- 0.6690527160116967
- 0.6691285425044121
- 0.6690210279632122
- 0.6693526269273555
- 0.6688195799893522
training_loss_epochs:
- 1.4516371207034333
- 1.2410590065286515
- 1.1805059977034305
- 1.1423090918267027
- 1.1143555095221134
- 1.0939199821746095
- 1.0776588826737505
- 1.0661824369050086
- 1.0580798093942885
- 1.0495920701229826
- 1.0420511808801205
- 1.0376842643986357
- 1.033692331707224
- 1.0285657726069715
- 1.025429997101743
- 1.0236046621140014
- 1.0204578232892016
- 1.0182049597197391
- 1.0178251010306338
- 1.0161869804275796
- 1.0144057925077195
- 1.013753962580194
- 1.0130259359770633
- 1.0135009582372423
- 1.0133841850656145
- 1.0147720731319265
- 1.0133158742113317
- 1.0131990699691975
- 1.0125124706866893
- 1.0125295800097445
- 1.0116015911736387
- 1.0118712373236391
- 1.012541048133627
- 1.0121195739254039
- 1.0121004509164933
- 1.0115675174175425
- 1.0117263383687811
- 1.012195295539308
- 1.0126697687392539
- 1.0105900874163243
- 1.0113687449313224
- 1.012281607630405
- 1.01200321716197
- 1.0120107031883077
- 1.011709238430287
- 1.0117909151822964
- 1.0118608233142405
- 1.0125250729474615
- 1.0118268280587297
- 1.0113235828724314
- 1.012257270356442
- 1.0114723186543648
- 1.012279296999282
- 1.010804215580859
- 1.011536810626375
- 1.0116375727856413
- 1.0112664035026064
- 1.0115371895597336
- 1.0116583857130497
- 1.011611314086204
training_time: 315.18135499954224
user_level: 1
validation_accuracy_epochs:
- 0.5770999235766274
- 0.5941074405397687
- 0.6006687836987632
- 0.6074569693633488
- 0.612624717610223
- 0.61692961198943
- 0.6190450234072549
- 0.6256933041981289
- 0.6267069748469761
- 0.6296060127871377
- 0.6277445937905993
- 0.6313294172286987
- 0.6306709817477635
- 0.6327471605369023
- 0.6325186235564095
- 0.633001331772123
- 0.6324939259460994
- 0.6332287277494159
- 0.6348363586834499
- 0.6362301664693015
- 0.6372176153319222
- 0.638408533164433
- 0.6371544556958335
- 0.638497165271214
- 0.639675246817725
- 0.6360657342842647
- 0.6384594525609698
- 0.6390671602317265
- 0.6392181856291634
- 0.6389276300157819
- 0.6401556943144117
- 0.6397248421396527
- 0.6399526170321873
- 0.6395228973456791
- 0.6383962844099317
- 0.639611520937511
- 0.639990895986557
- 0.6396997656141009
- 0.6398770000253405
- 0.6396748806749072
- 0.6397127679416111
- 0.6396999486855098
- 0.6396997613566262
- 0.6396490335464478
- 0.6394721823079246
- 0.6396364101341793
- 0.6395227100167956
- 0.6397248378821782
- 0.6399269785199847
- 0.6398511741842542
- 0.6391942458493369
- 0.639307941709246
- 0.6395479738712311
- 0.6392321416309902
- 0.6395100780895778
- 0.6395353376865387
- 0.639914333820343
- 0.6397374783243451
- 0.6395227100167956
- 0.6392953055245536
validation_loss_epochs:
- 1.3713876179286413
- 1.2764770303453719
- 1.2356558527265276
- 1.2006351692335946
- 1.1717934438160487
- 1.1577658482960291
- 1.1417424849101476
- 1.1338122912815638
- 1.1217316729681832
- 1.1132711257253374
- 1.1095235858644759
- 1.1016653691019331
- 1.1018866470881872
- 1.0952716640063695
- 1.0932592494147164
- 1.092782471861158
- 1.0874350326401847
- 1.0888150930404663
- 1.0849729861531938
- 1.0857043266296387
- 1.0811067478997367
- 1.0786169682230269
- 1.0839267032487052
- 1.0802127293178014
- 1.0819541301046098
- 1.0840512599263872
- 1.0843698382377625
- 1.0810639091900416
- 1.0802340252058846
- 1.0789486765861511
- 1.0804052480629511
- 1.0818063616752625
- 1.0796295404434204
- 1.0804139971733093
- 1.0812272003718786
- 1.080618747643062
- 1.0800442014421736
- 1.0802602171897888
- 1.0798272235052926
- 1.080068200826645
- 1.0801670040403093
- 1.0800520181655884
- 1.0801703248705183
- 1.0799270016806466
- 1.0802504590579443
- 1.080077886581421
- 1.0800313012940543
- 1.0801346812929427
- 1.0794321511472975
- 1.0795264158930098
- 1.08086530651365
- 1.0808509928839547
- 1.0805914487157549
- 1.0810312032699585
- 1.080083191394806
- 1.080321192741394
- 1.0796827035290855
- 1.0798775724002294
- 1.0803513441767012
- 1.080625627722059
virtual_batch_multiplier: 0
vocab_size: 10000
