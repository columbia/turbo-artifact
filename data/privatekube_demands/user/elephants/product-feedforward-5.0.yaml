accuracy: 0.602634910069689
adaptive_batch_size: 1
alphas:
- 1.5
- 1.75
- 2
- 2.5
- 3
- 4
- 5
- 6
- 8
- 16
- 32
- 64
batch_size: 124
best_alpha: 8.0
delta: 1.0e-09
device: cuda
dp: 1
dp_eval: 0
dropout: 0.25
dynamic_clipping: 0
embedding_dim: 100
epsilon: 4.9670336217042745
hidden_dim: 100
hidden_dim_1: 185
hidden_dim_2: 150
learning_rate: 0.01
learning_rate_scheduler: 1
loss: 1.4816608179973652
max_grad_norm: 1.0
max_text_len: 60
model: feedforward
n_blocks: 200
n_blocks_test: 200
n_epochs: 60
n_train_users: 15554
n_trainable_parameters: 48246
n_workers: 6
noise: 1.2078029632568361
non_dp_batch_size: 256
per_layer_clipping: 0
rdp_epsilons:
- 0.3502699785876049
- 0.4096805233886432
- 0.46939612566888367
- 0.5897586919379566
- 0.7113911412729209
- 0.9586090171400296
- 1.211358224253727
- 1.4699880706182387
- 2.006567073569073
- 2514.17506167215
- 44852.65014539943
- 127706.48618075538
target_epsilon: 5.0
task: product
test_size: 39077
timeframe_days: 0
total_time: 85.14428186416626
train_size: 64934
training_accuracy_epochs:
- 0.42606450176239014
- 0.5700644981861115
- 0.599290302991867
- 0.6092903032302857
- 0.6138709502220154
- 0.6264515933990479
- 0.6243870768547058
- 0.6301290128231049
- 0.6323870768547059
- 0.6336774005889892
- 0.6345161123275757
- 0.636645143032074
- 0.638580626487732
- 0.6398064339160919
- 0.6374838500022888
- 0.6394838509559632
- 0.6381290111541748
- 0.6410322408676148
- 0.6416128854751587
- 0.6441935276985169
- 0.6416128840446472
- 0.6401935272216797
- 0.6443870792388916
- 0.6437419152259827
- 0.6426451444625855
- 0.6427096552848816
- 0.6457419190406799
- 0.6403870792388916
- 0.6396773993968964
- 0.642903205871582
- 0.642387077331543
- 0.6428386869430542
- 0.6426451439857483
- 0.6414838538169861
- 0.6425806269645691
- 0.6433548226356506
- 0.6413548197746277
- 0.6453548197746277
- 0.6432257885932923
- 0.6447741758823394
- 0.6432257876396179
- 0.6417419180870056
- 0.6407741723060608
- 0.6410967578887939
- 0.6427096581459045
- 0.642967722415924
- 0.6427096557617188
- 0.641612886428833
- 0.6445806288719177
- 0.6423225598335266
- 0.6411612696647644
- 0.6417419176101684
- 0.6425806260108948
- 0.6433548173904419
- 0.6430322413444519
- 0.6442580440044403
- 0.6432257857322693
- 0.6450322391986847
- 0.6439999785423279
- 0.6442580454349518
training_loss_epochs:
- 1.9184454917907714
- 1.4641747064590453
- 1.4212152633666992
- 1.4313562755584717
- 1.4409249477386474
- 1.4430726432800294
- 1.4572889938354492
- 1.407742745399475
- 1.3896129388809204
- 1.3836806812286377
- 1.3647801070213317
- 1.3583999843597412
- 1.356510085105896
- 1.36505757522583
- 1.356771475315094
- 1.3551308546066285
- 1.353023232460022
- 1.3301715531349183
- 1.3316253185272218
- 1.3209883460998535
- 1.3260672731399537
- 1.3332401385307313
- 1.3295538630485535
- 1.3320062398910522
- 1.309942726612091
- 1.320265402317047
- 1.3258941640853883
- 1.3331722049713135
- 1.3181077542304993
- 1.3140344319343567
- 1.316450078010559
- 1.3175223956108093
- 1.3240717549324035
- 1.3305509767532349
- 1.3190329437255859
- 1.316414264202118
- 1.3242018642425537
- 1.3171858520507813
- 1.320355263710022
- 1.3216508264541627
- 1.3273474955558777
- 1.3242927165031433
- 1.335723207950592
- 1.3258811206817627
- 1.3158455228805541
- 1.322755250453949
- 1.3243582062721253
- 1.328880892753601
- 1.3090600762367248
- 1.3292817463874818
- 1.3275695991516114
- 1.3226828832626343
- 1.3200756263732911
- 1.3266987380981445
- 1.3210375180244447
- 1.3136418027877808
- 1.3284444160461426
- 1.3142218408584594
- 1.3137434315681458
- 1.3150361137390136
training_time: 61.231207847595215
user_level: 1
validation_accuracy_epochs:
- 0.5322580455609087
- 0.5510752504726626
- 0.5636031464585718
- 0.57240311126664
- 0.5890647001423925
- 0.5876191747638414
- 0.5731131901156228
- 0.5905355887030656
- 0.5851085218337347
- 0.5917528744013805
- 0.593959205555466
- 0.5932998406999516
- 0.5956329700519454
- 0.5924376010332467
- 0.6005781950815668
- 0.6018462006775838
- 0.597890022790657
- 0.5953286475730393
- 0.5958104934894813
- 0.5982450623557253
- 0.5973067379222726
- 0.6010600367806992
- 0.600476750225391
- 0.6027591633346846
- 0.6002485116697708
- 0.6028098843570026
- 0.6032410078453567
- 0.6007049952475529
- 0.5999695459064448
- 0.6030381245995468
- 0.6013136354257476
- 0.5984733017548075
- 0.6012375576878494
- 0.5997413104435183
- 0.6012375571255414
- 0.6026577235392805
- 0.5996652296129262
- 0.5996652307375422
- 0.5996652287694643
- 0.6011614771384113
- 0.5996652324244661
- 0.5996652290506183
- 0.6041539719082275
- 0.5981689820874412
- 0.6011614793876432
- 0.5981689798382094
- 0.5996652293317722
- 0.5981689851801351
- 0.6011614760137954
- 0.6026577249450503
- 0.5996652301752342
- 0.6026577246638963
- 0.6026577215712026
- 0.5996652324244661
- 0.6026577229769725
- 0.6011614771384113
- 0.6011614782630272
- 0.5996652318621581
- 0.5996652279260024
- 0.5996652315810042
validation_loss_epochs:
- 1.594723625003167
- 1.523423526646956
- 1.6048546804572053
- 1.5659391632619895
- 1.547750323448541
- 1.558016061782837
- 1.7237337299113005
- 1.550853597667982
- 1.5562238535791073
- 1.518721288105227
- 1.5005626925882303
- 1.5349704870637857
- 1.4847681376169313
- 1.509176232904758
- 1.49140709104403
- 1.473040004946151
- 1.4687719137038824
- 1.4906312986364905
- 1.4833914839996483
- 1.489901288500372
- 1.480928802265311
- 1.4572706093203347
- 1.4616573273010973
- 1.4368027746677399
- 1.456240659052471
- 1.4534918618089747
- 1.4554328148797997
- 1.4598002085145914
- 1.4639487277786687
- 1.4525612227196962
- 1.4572360065748107
- 1.4735577882460829
- 1.4611792114545714
- 1.4639570533104662
- 1.46292226831868
- 1.456325316204215
- 1.461492287662794
- 1.4642024996145717
- 1.4564792047131736
- 1.4712445792162194
- 1.4629897621442687
- 1.456679325621083
- 1.4500579341262017
- 1.47366356962132
- 1.464329795455033
- 1.4666718593183554
- 1.4676682532958265
- 1.461115077981409
- 1.4584712059992664
- 1.4550319055341325
- 1.4653951823711395
- 1.4553112275195572
- 1.455207402413746
- 1.4675305474479243
- 1.4570036845387153
- 1.4598617846111082
- 1.456515557361099
- 1.467586115846094
- 1.4658692687187556
- 1.4627475671048433
virtual_batch_multiplier: 0
vocab_size: 10000
