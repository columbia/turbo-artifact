accuracy: 0.7152004994844136
adaptive_batch_size: 1
alphas:
- 1.5
- 1.75
- 2
- 2.5
- 3
- 4
- 5
- 6
- 8
- 16
- 32
- 64
batch_size: 172
best_alpha: 6.0
delta: 1.0e-09
device: cuda
dp: 1
dp_eval: 0
dropout: 0.25
dynamic_clipping: 0
embedding_dim: 100
epsilon: 4.973310775959097
hidden_dim: 100
hidden_dim_1: 240
hidden_dim_2: 195
learning_rate: 0.01
learning_rate_scheduler: 1
loss: 0.5763536958318007
max_grad_norm: 1.0
max_text_len: 50
model: bow
n_blocks: 100
n_blocks_test: 200
n_epochs: 15
n_trainable_parameters: 101
n_workers: 6
noise: 0.8721129608154297
non_dp_batch_size: 256
per_layer_clipping: 0
rdp_epsilons:
- 0.17404449639352534
- 0.2044329298793226
- 0.235262877337346
- 0.2983364717875588
- 0.3634716294305238
- 0.5010911308607984
- 0.6519709938267084
- 0.8286576085698144
- 6.459902451472978
- 12957.974722602235
- 40552.514694284815
- 95044.6177921964
target_epsilon: 5.0
task: sentiment
test_size: 39077
timeframe_days: 0
total_time: 35.68701672554016
train_size: 29727
training_accuracy_epochs:
- 0.6529205001370851
- 0.656368309675261
- 0.6712412137624829
- 0.6895957242610843
- 0.7006827984438386
- 0.704502428686896
- 0.7083220585834148
- 0.710045964565388
- 0.7128515357888022
- 0.7163669508556987
- 0.7162317408378734
- 0.7191725193068038
- 0.7189697047998739
- 0.7192401234493699
- 0.721471061886743
training_loss_epochs:
- 0.6584594007148299
- 0.6193377798379853
- 0.5991328268550163
- 0.5836104806079421
- 0.5756207422115082
- 0.5716155050105827
- 0.5694228298095769
- 0.566465781525124
- 0.5643699484509092
- 0.562952202419902
- 0.5620151103235954
- 0.5601025182147359
- 0.5601101964712143
- 0.5594625467824381
- 0.5575552730019703
training_time: 13.100567102432251
user_level: 0
validation_accuracy_epochs:
- 0.6533970621071363
- 0.6620694171441229
- 0.6811384072429255
- 0.6881350663147474
- 0.6947212932925475
- 0.698685906435314
- 0.7087558962796864
- 0.7079763286992123
- 0.7100988877447028
- 0.7087619696792803
- 0.71106545078127
- 0.7070449810279044
- 0.7146827782455244
- 0.7196758805136931
- 0.7092913891139784
validation_loss_epochs:
- 0.6311369224598533
- 0.6098061462766245
- 0.5922194887932978
- 0.5865784491363325
- 0.580360629840901
- 0.578299374172562
- 0.5693653476865668
- 0.5721180940929212
- 0.5740569214287558
- 0.5785158943188818
- 0.5747687510753933
- 0.5840405115955755
- 0.5726150595828107
- 0.5671936806879545
- 0.5840368447335142
virtual_batch_multiplier: 0
vocab_size: 10000
